{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde58157-7750-45b9-a343-b46e3c74b5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dependencies import\n",
    "\n",
    "#common\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pathlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "# ML\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Dropout,\n",
    "    concatenate, Flatten, Dense, UpSampling2D,\n",
    "    BatchNormalization\n",
    ")\n",
    "\n",
    "# my\n",
    "from create_logger import *\n",
    "import custom_modules as dw\n",
    "import models\n",
    "\n",
    "logger = logging.getLogger(f'main.ae_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5d81-3819-40f9-a170-0fa8cb48a598",
   "metadata": {},
   "source": [
    "# Настраиваемые параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3816d-283a-4b51-852c-4a4e077123d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_MODEL = pathlib.Path(f'test/')\n",
    "MODEL_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bd2c5-e80b-4e8a-a440-c0789a175d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSHIFT = 200 # сдвиг по оси х для визуализации\n",
    "\n",
    "# настройка параметров выборок\n",
    "dataset_desc = {'train': [dw.DataPart(run_name='run_1', height=60),\n",
    "                          dw.DataPart(run_name='run_2', height=60)],\n",
    "                'val': [dw.DataPart(run_name='run_1', xy=(0,60), height=20),\n",
    "                        dw.DataPart(run_name='run_2', xy=(0,60), height=20)],\n",
    "                'test': [dw.DataPart(run_name='run_1', xy=(0,80)),\n",
    "                        dw.DataPart(run_name='run_2', xy=(0,80))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aae19-7043-4dc0-a4ff-b3d50e0ea9d8",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8700946-113d-41cc-a1a3-99f832c23c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_as_squeezed_numpy_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        gen = func(*args, **kwargs)\n",
    "        return np.squeeze(np.array(list(gen)))\n",
    "    return wrapper\n",
    "    \n",
    "@get_dataset_as_squeezed_numpy_decorator\n",
    "def get_dataset(df: pd.DataFrame, descs: list[dw.DataPart]):\n",
    "    \"\"\"\n",
    "    Подготовить датасет на основании списка описаний dw.DataPart.\n",
    "    Готовит датасет для одной из 3 выборок за раз - тренировочной, тестовой или валидационной\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = dw.cast_df_to_2d(df)\n",
    "    for i, ((_, time), (_, amp)) in enumerate(zip(df['Time'].items(), df['Amplitude'].items())):\n",
    "        df['Time_x_Amplitude', i] = time * amp\n",
    "    df = df.drop(['Time', 'Amplitude', 'DefectDepth'], axis=1)\n",
    "    arr = dw.standardize_data(df.to_numpy())\n",
    "    df = pd.DataFrame(data=arr, index=df.index, columns=df.columns)\n",
    "    df = dw.cast_df_to_3d(df)\n",
    "    \n",
    "    generators_list = []\n",
    "    for desc in descs:\n",
    "        temp_df = df.copy()\n",
    "        if not desc.run_name is None:\n",
    "            temp_df = temp_df.loc[desc.run_name]\n",
    "        temp_df = dw.crop_df(temp_df, desc.xy, desc.width, desc.height)\n",
    "        generators_list.append(dw.get_crop_generator(dw.df_to_numpy(temp_df), \n",
    "                                                     desc.crop_size, \n",
    "                                                     desc.crop_step, \n",
    "                                                     desc.augmentations))\n",
    "    return itertools.chain(*generators_list)    \n",
    "\n",
    "\n",
    "\n",
    "# def generator_to_squeezed_numpy(gen):\n",
    "#     return  np.squeeze(np.array(list(gen)))\n",
    "\n",
    "def draw_plot(data: list[dict], title = 'Результат одного замера УЗ-датчика', x_label = 'Время', y_label = 'Амплитуда', fontsize = 25, path_to_save = None):\n",
    "    \"\"\"\n",
    "    Нарисовать график\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    data: list[dict]\n",
    "        Список словарей. Каждый словарь хранит\n",
    "        данные и все мараметры для рисования конкретного графика.\n",
    "        Все графики будут нарисованы на 1 полотне. 'data' параметр обязателен в \n",
    "        каждом словаре. Он хранит список из 1 или 2 массивов - это x и y для \n",
    "\n",
    "    Пример: \n",
    "    time = [1,2,3,4,5]\n",
    "    amp = [4,-4,5,-5,6]\n",
    "    draw_plot([{'data':[time, amp], 'marker':'o', 'lw':3, 'label':'Исходные данные', 'ms':10, 'mfc':'black'}])\n",
    "    Все ключевые слова взяты из функции plt.plot()\n",
    "\n",
    "    plt.plot()\n",
    "    Если указать в словаре только 'data' параметр без остальных, то оформление будет сделано автоматом\n",
    "\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(10)\n",
    "    \n",
    "    fig.set_facecolor('#37474f')\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "\n",
    "    for item in data:\n",
    "        if not 'data' in item:\n",
    "            raise ValueError('Каждый словарь должен содержать ключ \"data\"')\n",
    "        if list(item.keys()) == ['data']:\n",
    "            ax.plot(*item['data'], marker='o', lw=3, label='Исходные данные', ms=10, mfc='black')\n",
    "        else:\n",
    "            ax.plot(*item['data'], **item)\n",
    "    \n",
    "\n",
    "    fig.suptitle(title, fontsize=fontsize+5, c='#cacaca')\n",
    "    ax.legend(fontsize = fontsize, labelcolor='#cacaca', facecolor='black')\n",
    "    ax.set_xlabel(x_label, fontsize=fontsize, c='#cacaca')\n",
    "    ax.set_ylabel(y_label, fontsize=fontsize, c='#cacaca')\n",
    "    \n",
    "    ax.tick_params(axis='both', labelsize = fontsize)\n",
    "    ax.grid(True, which='major', axis='both', lw=1.5)\n",
    "    ax.grid(True, which='minor', axis='both', ls='--')\n",
    "    \n",
    "    ax.minorticks_on()\n",
    "    \n",
    "    ax.tick_params(axis = 'both', which = 'major', length = 8, width = 4, colors='#cacaca')\n",
    "    ax.tick_params(axis = 'both', which = 'minor', length = 4, width = 2, labelleft=True, colors='#cacaca', labelsize=fontsize-8)\n",
    "    \n",
    "    #ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "    #ax.yaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "    #ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.3f\"))\n",
    "    #ax.yaxis.set_minor_formatter(FormatStrFormatter(\"%.3f\"))\n",
    "    \n",
    "    ax.set_facecolor\n",
    "    #plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.1, hspace=0.1)\n",
    "    if not path_to_save is None:\n",
    "        plt.savefig(path_to_save, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a059ca0-3552-4d9d-b6c9-ad81183cf0b1",
   "metadata": {},
   "source": [
    "# Чтение и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667bd61-4cca-4c3d-a9b9-b055d924e1a7",
   "metadata": {},
   "source": [
    "## Оригинальные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280f76f-bbd8-49c5-aa14-40664200a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dw.get_data_df('data/original_data')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a1115-94e9-4c13-8a6c-675f0f6f75a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(dw.cast_df_to_2d(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008cdc-b9fd-4fdf-a6b4-4b0ef82d9fd6",
   "metadata": {},
   "source": [
    "## Данные после перемножения времен на амплитуды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114c5c8-0d55-443b-8813-3db4483fc393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = dw.cast_df_to_2d(df).copy()\n",
    "for i, ((_, time_val), (_, amp_val)) in enumerate(zip(test_df['Time'].items(), test_df['Amplitude'].items())):\n",
    "    test_df['Time_x_Amplitude', i] = time_val * amp_val\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f61e3-2f20-48d8-8e97-07bb454842fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# нарисовать графики до обработки\n",
    "for i in range(10):\n",
    "    draw_plot([{'data':[test_df.loc['run_1',0,i]['Time'], test_df.loc['run_1',0,i]['Amplitude']]},\n",
    "               \n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11fb65-cf2b-4e37-9b53-beee964c7f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# нарисовать графики после перемножения времен и амплитуд\n",
    "for i in range(10):\n",
    "    draw_plot([{'data':[test_df.loc['run_1',0,i]['Time_x_Amplitude']]},\n",
    "              \n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbd875-a840-4f11-9257-9991d4a2e3b8",
   "metadata": {},
   "source": [
    "## Визуализация частей выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87bc40-f9cd-4eeb-94b5-0a83ec92b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show parts took for learning\n",
    "all_rects = []\n",
    "rects_colors = {'train':'red', 'val':'green', 'test':'yellow', 'all':'orange'}\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df = temp_df.map(lambda x: x[-1])\n",
    "temp_df = dw.roll_df(temp_df, XSHIFT, 1)\n",
    "\n",
    "for dataset_name, descs in dataset_desc.items():\n",
    "    for desc in descs:\n",
    "        xy = desc.xy\n",
    "        width = desc.width\n",
    "        height = desc.height\n",
    "        run_name = desc.run_name\n",
    "    \n",
    "        if desc.run_name is None:\n",
    "            if width is None:\n",
    "                width = temp_df.shape[1]\n",
    "            if height is None:\n",
    "                height = temp_df.shape[0]\n",
    "        else:\n",
    "            if height is None:\n",
    "                height = temp_df.loc[desc.run_name].shape[0] - xy[1]\n",
    "            if width is None:\n",
    "                width = temp_df.loc[desc.run_name].shape[1]\n",
    "                \n",
    "            for i, item in enumerate(df.index):\n",
    "                if item[0] == desc.run_name:\n",
    "                    xy = (xy[0], i+xy[1])\n",
    "                    break\n",
    "                    \n",
    "        all_rects.append(Rectangle(xy, width, height, facecolor=rects_colors[dataset_name], alpha=0.5))\n",
    "\n",
    "if all_rects:\n",
    "    dw.draw_defects_map_with_rectangles_owerlap(temp_df, all_rects, title = f'The parts took for learning from {set(temp_df.index.get_level_values(\"File\"))} {rects_colors}')\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5a998-c02b-44ee-bc9f-9ade027e0a3b",
   "metadata": {},
   "source": [
    "# Распределение данных по выборкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828dd63-9bf0-40fa-ba91-2ad4a8f537b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = get_dataset(df, dataset_desc['train'])\n",
    "val= get_dataset(df, dataset_desc['val'])\n",
    "test = get_dataset(df, dataset_desc['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e5f9f-ee80-4c0f-8968-f532ce97642a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.debug('Dataset parts shapes')\n",
    "logger.debug(f'{train.shape=}')\n",
    "logger.debug(f'{val.shape=}')\n",
    "logger.debug(f'{test.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f26eb-52a9-4835-bacb-f907f8197bb3",
   "metadata": {},
   "source": [
    "# Создание и обучение модели автокодировщика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d8e94-df0a-45bb-8556-6e04dcc6ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AE(learning_rate):\n",
    "    solver = keras.optimizers.Adam(learning_rate) # оптимизатор\n",
    "    loss_funcs = keras.losses.MeanSquaredError() # функция потерь/ошибки\n",
    "    metrics = [keras.metrics.MeanSquaredError(name='MeanSquaredError')] # отслеживаемые метрики\n",
    "\n",
    "    enc_input = layers.Input((32,), name='enc_input')\n",
    "    d_1 = layers.Dense(32, activation='tanh')(enc_input)\n",
    "    \n",
    "    d_2 = layers.Dense(16, activation='tanh')(d_1)\n",
    "    d_3 = layers.Dense(16, activation='tanh')(d_1)\n",
    "    \n",
    "    hidden_state_output = layers.Dense(8, activation='tanh', name='hidden_state_output')(concatenate([d_2, d_3], axis=1))\n",
    "    \n",
    "    d_5 = layers.Dense(16, activation='tanh')(hidden_state_output)\n",
    "    d_6 = layers.Dense(16, activation='tanh')(hidden_state_output)\n",
    "    \n",
    "    dec_output = layers.Dense(32, activation='tanh', name='dec_output')(concatenate([d_5, d_6], axis=1))\n",
    "    \n",
    "    model = keras.Model(enc_input, dec_output, name='AE')\n",
    "    model.compile(optimizer=solver, loss=loss_funcs, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d20658-5e0a-4bea-9b20-ad18ac43fe89",
   "metadata": {},
   "source": [
    "## Вывести параметры модели и её граф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da92fca-9a8e-443d-b021-bf953280d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations = 'sigmoid' #'relu' 'x' 'tanh'\n",
    "# losses = \n",
    "model = get_AE(0.001)\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=200,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d91a2-ab04-4ade-b0b7-f1cf97742615",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a506df-3c83-4ee5-a1f7-473c616e9f2f",
   "metadata": {},
   "source": [
    "## Не настраиваемые параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392190-9117-4bd1-a356-4b85b9d7b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# размер входных и выходных данных\n",
    "ENCODED_SIZE = min([layer.output.shape[1] for layer in model.layers]) \n",
    "DECODED_SIZE = model.layers[-1].output.shape[1]\n",
    "PATH_TO_SAVE_MODEL = PATH_TO_SAVE_MODEL/f'AE/encoded_to_{ENCODED_SIZE}'\n",
    "PATH_TO_SAVE_MODEL_PROGRESS = PATH_TO_SAVE_MODEL/'logs'\n",
    "MODEL_NUMBER = 1\n",
    "MIN_TRAIN_LOSS = 1\n",
    "MIN_VAL_LOSS = 1\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_MODEL):\n",
    "    os.makedirs(PATH_TO_SAVE_MODEL)\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_MODEL_PROGRESS):\n",
    "    os.makedirs(PATH_TO_SAVE_MODEL_PROGRESS)\n",
    "\n",
    "# все имеющиеся модели с такими же ENCODED_SIZE и DECODED_SIZE\n",
    "all_ae_models = [path.name for path in PATH_TO_SAVE_MODEL.parent.rglob('*.keras') \n",
    "                 if re.search(fr'in\\({DECODED_SIZE}\\)_hid\\({ENCODED_SIZE}\\)', path.name)]\n",
    "\n",
    "# если уже есть такая же архитектура модели сделать MODEL_VERSION такой же\n",
    "# а MODEL_NUMBER на 1 больше чем имеющаяся\n",
    "if all_ae_models:\n",
    "    min_train_loss = min([float(re.findall(fr'train=(.+),val', name)[0]) for name in all_ae_models])\n",
    "    min_val_loss = min([float(re.findall(fr',val=(.+),test', name)[0]) for name in all_ae_models])\n",
    "    min_model_number = max([int(re.findall(fr'id=v{MODEL_VERSION:04}n(\\d+)_', name)[0]) for name in all_ae_models])\n",
    "\n",
    "    if MODEL_NUMBER <= min_model_number:\n",
    "        MODEL_NUMBER = min_model_number+1\n",
    "        \n",
    "    if MIN_TRAIN_LOSS >= min_train_loss:\n",
    "        MIN_TRAIN_LOSS = min_train_loss\n",
    "\n",
    "    if MIN_VAL_LOSS >= min_val_loss:\n",
    "        MIN_VAL_LOSS = min_val_loss\n",
    "\n",
    "\n",
    "\n",
    "print(f'{ENCODED_SIZE=}')\n",
    "print(f'{DECODED_SIZE=}')\n",
    "print(f'{PATH_TO_SAVE_MODEL=}')\n",
    "print(f'{PATH_TO_SAVE_MODEL_PROGRESS=}')\n",
    "print(f'{MODEL_VERSION=}')\n",
    "print(f'{MODEL_NUMBER=}')\n",
    "print(f'{MIN_TRAIN_LOSS=}')\n",
    "print(f'{MIN_VAL_LOSS=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6d864-b783-4dad-b9f6-3726243f4449",
   "metadata": {},
   "source": [
    "### Настройка и создание коллбэков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74e1b4-c3bb-465d-b80f-a3246e00ff42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback_params = {\n",
    "    # остановка обучения если модель перестала учиться\n",
    "    'EarlyStopping': {\n",
    "        'monitor': 'val_loss', # отслеживаемый параметр \n",
    "        'min_delta': 0.00001, # минимальное улучшение параметра за cur_patience\n",
    "        'patience': 6, # кол-во эпох без улучшений\n",
    "        'restore_best_weights': False,  # сохранять ли веса нейронки с лучшими результатами\n",
    "    },\n",
    "\n",
    "    # уменьшение шага сходимости, если модель стала мендленно учиться\n",
    "    'ReduceLROnPlateau': {\n",
    "        'monitor' : 'loss', # отслеживаемый параметр \n",
    "        'factor' : 0.2, # множитель для расчета нового шага сходимости (new_learning_rate = old_learning_rate*RLPOP_factor)\n",
    "        'patience' : 3, # кол-во эпох без улучшений\n",
    "        'verbose' : 0, # выводить ли прогресс изменения шага сходимости в его процессее\n",
    "        'min_delta' : 0.0001, # порог изменения отслеживаемого значения\n",
    "        'cooldown' : 1, # количество эпох до возобновления работы после изменения шага сходимости\n",
    "        'min_lr' : 0# минимальное значение шага сходимости\n",
    "    },\n",
    "}\n",
    "\n",
    "# Создание и настройка колбэков\n",
    "callback_list = [] # массив колбэков до подачи в колбек \"callbacklist\"\n",
    "# остановка обучения если модель перестала учиться\n",
    "callback_list.append(keras.callbacks.EarlyStopping(**callback_params['EarlyStopping']))\n",
    "# уменьшение шага сходимости, если модель стала мендленно учиться\n",
    "callback_list.append(keras.callbacks.ReduceLROnPlateau(**callback_params['ReduceLROnPlateau']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a50d03-1745-4965-b962-b4f016d3cf44",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed8d62-0d28-4553-a89a-1995d719cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_learning_params(seed, lrate, batch_size, learn_time, history, callback_params, dataset_desc, train, val, test, path_to_save):\n",
    "    # начальные параметры обучения\n",
    "    learning_params_df = pd.DataFrame({'random_seed': [seed], \n",
    "                       'learning_rate': [lrate], \n",
    "                       'batch_size': [batch_size], \n",
    "                       'time_to_learn (seconds)': [learn_time]})\n",
    "    \n",
    "    # параметры процесса обучения\n",
    "    learn_df = pd.DataFrame.from_dict(history)\n",
    "    learn_df.index.name = 'epoch'\n",
    "    learn_df.columns.name = 'param'\n",
    "    \n",
    "    # параметры коллбэков\n",
    "    callback_df = pd.DataFrame.from_dict(callback_params)\n",
    "    callback_df.index.name = 'param'\n",
    "    callback_df.columns.name = 'callback_name'\n",
    "    \n",
    "    \n",
    "    # параметры для подготовки и чтения датасетов\n",
    "    descs = {key:[{**desc.model_dump(), 'desc_number':i} for i, desc in enumerate(item)] for key, item in dataset_desc.items()}\n",
    "    dataset_df_list = []\n",
    "    for key, item in descs.items():\n",
    "        for item_i in item:\n",
    "            temp = pd.DataFrame({key:pd.Series(item_i)}).T\n",
    "            temp.index.name = 'dataset_part'\n",
    "            dataset_df_list.append(temp)\n",
    "        \n",
    "    dataset_df = pd.concat(dataset_df_list, axis=0)\n",
    "    dataset_df = dataset_df.reset_index()\n",
    "    dataset_df = dataset_df.set_index(['dataset_part', 'run_name', 'desc_number'])\n",
    "    \n",
    "    # параметры подготовленных датасетов\n",
    "    prepared_dataset_df = pd.DataFrame({'Shape':pd.Series({'train': train.shape, 'val': val.shape, 'test': test.shape})})\n",
    "    prepared_dataset_df.index.name = 'dataset_part'\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(path_to_save) as writer:\n",
    "        learning_params_df.to_excel(writer, sheet_name = 'learning_start_params')\n",
    "        learn_df.to_excel(writer, sheet_name = 'learning_progress')\n",
    "        callback_df.to_excel(writer, sheet_name = 'callback_params')\n",
    "        dataset_df.to_excel(writer, sheet_name = 'dataset_params')\n",
    "        prepared_dataset_df.to_excel(writer, sheet_name = 'prepared_dataset_shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6f620-b611-4c93-a6b6-510df358561b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cont = False\n",
    "for seed in range(0, 200, 10):\n",
    "    print(f'Seed: {seed}',\"|\"*10)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    for lrate in [0.01,0.005,0.0025]:\n",
    "        print(f'Start learning rate: {lrate}',\"|\"*5)\n",
    "        for batch_size in [8,16,32,64]:\n",
    "            print(f'\\tBatch size: {batch_size}')\n",
    "\n",
    "            # get model\n",
    "            model = get_AE(lrate)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            history = model.fit(train, train,\n",
    "                            batch_size = batch_size, \n",
    "                            epochs = 80, \n",
    "                            verbose = 0, \n",
    "                            shuffle = True,\n",
    "                            validation_data = (val, val), \n",
    "                            callbacks = callback_list)\n",
    "            \n",
    "            cur_test_loss = model.evaluate(test, test, batch_size=batch_size, verbose=0)[0]\n",
    "\n",
    "            learn_time = time.time() - start\n",
    "            \n",
    "\n",
    "            cur_train_loss = history.history['loss'][-1]\n",
    "            cur_val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "            model_name = (\n",
    "                f\"id=v{MODEL_VERSION:04}n{MODEL_NUMBER:04}\" +\n",
    "                f\"_in({DECODED_SIZE})_hid({ENCODED_SIZE})\" + \n",
    "                f\"_loss_MSE=(train={cur_train_loss:.5f},\" + \n",
    "                f\"val={cur_val_loss:.5f},test={cur_test_loss:.5f})\" + \n",
    "                f\"_seed={seed}_lrate={lrate}_bach_size={batch_size}_tf={tf.__version__}\")\n",
    "\n",
    "\n",
    "            if cur_train_loss < MIN_TRAIN_LOSS and cur_val_loss < MIN_VAL_LOSS:\n",
    "                MIN_TRAIN_LOSS = cur_train_loss\n",
    "                MIN_VAL_LOSS = cur_val_loss\n",
    "                cont = True\n",
    "            if cur_val_loss < MIN_VAL_LOSS:\n",
    "                MIN_VAL_LOSS = cur_val_loss\n",
    "                cont = True\n",
    "            if cur_train_loss < MIN_TRAIN_LOSS:\n",
    "                MIN_TRAIN_LOSS = cur_train_loss\n",
    "                cont = True\n",
    "\n",
    "                \n",
    "            print(f'\\t\\tEpochs: {len(history.history[\"loss\"])}')\n",
    "            print(f\"\\t\\tloss_MSE=(train={cur_train_loss:.5f},val={cur_val_loss:.5f},test={cur_test_loss:.5f})\")\n",
    "            print(f'\\t\\tВремя на обучение модели: {learn_time}')\n",
    "            \n",
    "            if cont:\n",
    "               \n",
    "                save_learning_params(seed, lrate, batch_size, learn_time, history.history, callback_params, dataset_desc, train, val, test, PATH_TO_SAVE_MODEL_PROGRESS/f\"{model_name}_learning_data.xlsx\")\n",
    "                    \n",
    "                model.save(PATH_TO_SAVE_MODEL/f'{model_name}.keras')\n",
    "                \n",
    "                MODEL_NUMBER+=1\n",
    "                cont=False\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b744e-8c90-42b8-8944-dcf7bb27e978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

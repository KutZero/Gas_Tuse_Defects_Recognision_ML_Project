{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e9457-0887-469b-830b-6f7396be6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies import\n",
    "from common_dependencies import *\n",
    "from itertools import chain\n",
    "import logging\n",
    "logger = logging.getLogger('main.make_pred.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34591ee-be65-42f8-8e4b-a8d2f49cee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# детерминация случайных величин, отвечающих за выбор первоначальных весов и биасов\n",
    "tf.compat.v1.set_random_seed(290)\n",
    "tf.random.set_seed(290)\n",
    "\n",
    "# paths for files with original data\n",
    "RUNS = [2]\n",
    "XSHIFT = 200\n",
    "\n",
    "MODEL_VER = 14\n",
    "MODEL_NUM = 11\n",
    "EPOCH = 60\n",
    "\n",
    "model_pathes = [path for path in pathlib.Path(f'networks/CNN').rglob(f'*.keras') \n",
    "                     if re.search(rf'id=v0*{MODEL_VER}n0*{MODEL_NUM}.*epoch={EPOCH}*',path.name)]\n",
    "\n",
    "if len(model_pathes) == 1:\n",
    "    PATH_TO_MODEL = model_pathes[0]\n",
    "else:\n",
    "    print(model_pathes)\n",
    "    raise ValueError('Few or none model have been found instead of one')\n",
    "\n",
    "print(f'{PATH_TO_MODEL=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3135c-1cb6-4061-b326-664c758c1944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "load_model = keras.models.load_model(PATH_TO_MODEL)\n",
    "CROP_SIZE = load_model.inputs[0].shape[1]\n",
    "CROP_STEPS = [16] #[CROP_SIZE, CROP_SIZE//2, CROP_SIZE//4, CROP_SIZE//8]\n",
    "print(f'{CROP_SIZE=}')\n",
    "print(f'{CROP_STEPS=}')\n",
    "'''print(load_model.summary())\n",
    "tf.keras.utils.plot_model(\n",
    "    load_model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=200,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False,\n",
    ")''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f261d-0392-45a1-bd4f-82914c666f0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Новый алгоритм построения предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c553278-da06-4eea-950f-094c3ca5a36e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''def build_add_remove_1_prediction(extend_size_result_df: pd.DataFrame, orig_shape: tuple[int,int], \n",
    "                                  crop_size: int, crop_step: int, res: np.ndarray, tholds=[0.2,0.4,0.6,0.8], **kvargs):\n",
    "    \"\"\"\n",
    "    The algorithm that will iterate through the \n",
    "    ex_x_df size empty pandas.DataFrame like it is being dividing by crops\n",
    "    of given params and if the current prediction greater or equal to the current \n",
    "    thold from the tholds, so all cells of the empty dataframes that refer to the current\n",
    "    crop will be incremented by 1, otherwise decremented by 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extend_size_result_df: pandas.DataFrame\n",
    "        The empty dataframe of same shape as extended for prediction \n",
    "        and crop dividing one for build the prediction map on base to. \n",
    "    orig_shape: tuple[int,int]\n",
    "        The shape of ex_x_df before extending.\n",
    "    res: np.ndarray\n",
    "        The flat array of the model predictions for all crops of the\n",
    "        ex_x_df preprocessed before.\n",
    "    tholds: iterable\n",
    "        The thresholds to use in the algorithm.\n",
    "    \"\"\"\n",
    "    res_it = iter(res)\n",
    "    for i in range(0, extend_size_result_df.shape[0] - crop_size + 1, crop_step):\n",
    "        for j in range(0,  extend_size_result_df.shape[1] - crop_size + 1, crop_step):\n",
    "            temp_add = next(res_it)\n",
    "            for thold in tholds:\n",
    "                if temp_add >= thold:\n",
    "                    extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "                        extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: x+1)\n",
    "                else:\n",
    "                    extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "                        extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: x-1)\n",
    "                    \n",
    "            extend_size_result_df = pd.DataFrame(data=dw.normalize_data(extend_size_result_df.to_numpy()), \n",
    "                                         index=extend_size_result_df.index, \n",
    "                                         columns=extend_size_result_df.columns)\n",
    "            orig_size_result_df = extend_size_result_df.iloc[crop_size-1:, crop_size-1:].iloc[:orig_shape[0],:orig_shape[1]]\n",
    "\n",
    "            info = {'crop_size': crop_size, 'crop_step': crop_step, 'thold': thold}\n",
    "            \n",
    "            yield orig_size_result_df, extend_size_result_df, {**info, **kvargs}''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868feb91-a08d-485d-b46f-f080f61695a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''def build_add_to_array_prediction(extend_size_result_df: pd.DataFrame, orig_shape: tuple[int,int], \n",
    "                                  crop_size: int, crop_step: int, res: np.ndarray, **kvargs): \n",
    "    \"\"\"\n",
    "    The algorithm that will iterate through the \n",
    "    ex_x_df size empty pandas.DataFrame like it is being dividing by crops\n",
    "    of given params and add every prediction value to arrays in each cell that \n",
    "    refer to the current crop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extend_size_result_df: pandas.DataFrame\n",
    "        The empty dataframe of same shape as extended for prediction \n",
    "        and crop dividing one for build the prediction map on base to. \n",
    "    orig_shape: tuple[int,int]\n",
    "        The shape of ex_x_df before extending.\n",
    "    res: np.ndarray\n",
    "        The flat array of the model predictions for all crops of the\n",
    "        ex_x_df preprocessed before.\n",
    "    \"\"\"\n",
    "    extend_size_result_df.map(lambda x: list())\n",
    "    res_it = iter(res)\n",
    "    for i in range(0, extend_size_result_df.shape[0] - crop_size + 1, crop_step):\n",
    "        for j in range(0,  extend_size_result_df.shape[1] - crop_size + 1, crop_step):\n",
    "            temp_add = next(res_it)\n",
    "            extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "               extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: [*x,temp_add])\n",
    "            \n",
    "    orig_size_result_df = extend_size_result_df.iloc[crop_size-1:, crop_size-1:].iloc[:orig_shape[0],:orig_shape[1]]\n",
    "\n",
    "    info = {'crop_size': crop_size, 'crop_step': crop_step, \n",
    "            'xshift': xshift, 'thold': thold}\n",
    "    \n",
    "    yield orig_size_result_df, extend_size_result_df, {**info, **kvargs}''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a19928-3f2f-4a63-9918-c33eb6a33d77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''def build_pred_map(ex_x_df, orig_shape: tuple[int,int], res, crop_size: int, crop_step: int, method: str='add_remove_1', **kvargs):\n",
    "    \"\"\"\n",
    "    Build prediction map for the ex_x_df data and res_it predictions iterator with\n",
    "    defined crop step and crop size by method defined by the method param.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ex_x_df: pandas.DataFrame\n",
    "        The extended for prediction and crop dividing read data \n",
    "        for build the prediction map on base to. The prediction \n",
    "        map should be the pandas.DataFrame of the same shape and\n",
    "        column and index names as the ex_x_df.\n",
    "    orig_shape: tuple[int,int]\n",
    "        The shape of ex_x_df before extending.\n",
    "    res: np.ndarray\n",
    "        The flat array of the model predictions for all crops of the\n",
    "        ex_x_df preprocessed before.\n",
    "    crop_size: int\n",
    "        The crop size used to preprocess read data.\n",
    "    crop_step: int\n",
    "        The crop step used to preprocess read data.\n",
    "    method: str\n",
    "        Which of two method for predictions aggregation to use.\n",
    "        May be either 'add_remove_1' or 'add_to_array'.\n",
    "        If 'add_remove_1' the algorithm will iterate through the \n",
    "        ex_x_df size empty pandas.DataFrame like it is being dividing by crops\n",
    "        of given params and if the current prediction greater or equal to the current \n",
    "        thold from the tholds, so all cells of the empty dataframes that refer to the current\n",
    "        crop will be incremented by 1, otherwise decremented by 1.\n",
    "        If 'add_to_array' the algorithm will iterate through the \n",
    "        ex_x_df size empty pandas.DataFrame like it is being dividing by crops\n",
    "        of given params and add every prediction value to arrays in each cell that \n",
    "        refer to the current crop.\n",
    "    **kvargs\n",
    "        Any additional params that may be defined in the build_add_remove_1_prediction() or\n",
    "        build_add_to_array_prediction().\n",
    "    \"\"\"\n",
    "    if not method in ('add_remove_1','add_to_array'):\n",
    "        raise ValueError('The method param should be one of [\\'add_remove_1\\',\\'add_to_array\\']')\n",
    "        \n",
    "    extend_size_result_df = pd.DataFrame(data=0.0, \n",
    "                             columns=ex_x_df.columns, \n",
    "                             index=ex_x_df.index)\n",
    "    \n",
    "    if method == 'add_remove_1':\n",
    "        return build_add_remove_1_prediction(extend_size_result_df, orig_shape, crop_size, crop_step, res, method=method, **kvargs)\n",
    "    else:\n",
    "        return build_add_to_array_prediction(extend_size_result_df, orig_shape, crop_size, crop_step, res, method=method, **kvargs)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50bf77-50ac-4e68-9262-a8f276c4a69a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''def get_xshifts_data_generator(model, x_df: pd.DataFrame, y_df: pd.DataFrame, xshifts, crop_step: int, buffer: int=15_000, **kvargs):\n",
    "    \"\"\"\n",
    "    For any crop step there are crop_step options of result prediction map due\n",
    "    to different start point. So the function enumerate all crop_sizes and xshifts.\n",
    "    Futhermore data preprocessing deffers for all combinations. Then the function\n",
    "    calls build_pred_map() to build prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tensorflow.keras.Model\n",
    "        The tensorflow model to use in prediction.\n",
    "    x_df: pandas.DataFrame\n",
    "        The read data for passing into the model inputs.\n",
    "    y_df: pandas.DataFrame\n",
    "        The read data for cheching model predictions quality.\n",
    "    xshifts: iterable\n",
    "        The any flat iterable with all xshifts for the crop_step.\n",
    "    crop_step: int\n",
    "        The crop step to use in data preprocessing and prediction\n",
    "        map building.\n",
    "    buffer: int\n",
    "        How many crops the PC can store in RAM at one moment. The bigger - \n",
    "        the faster it works.\n",
    "    **kvargs\n",
    "        Any additional params that may be defined in the build_pred_map().\n",
    "    \"\"\"\n",
    "    crop_size = model.inputs[0].shape[1]\n",
    "    for xshift in xshifts:\n",
    "        x_df = dw.roll_df(x_df, xshift, 1)\n",
    "        y_df = dw.roll_df(y_df, xshift, 1)\n",
    "        orig_shape = x_df.shape\n",
    "    \n",
    "        ex_x_df = dw.extend_df_for_prediction(x_df, crop_size)\n",
    "        ex_x_df = dw.match_df_for_crops_dividing(ex_x_df, crop_size, crop_step)\n",
    "        ex_y_df = dw.extend_df_for_prediction(y_df, crop_size)\n",
    "        ex_y_df = dw.match_df_for_crops_dividing(ex_y_df, crop_size, crop_step)\n",
    "        \n",
    "        x_arr = dw.df_to_numpy(ex_x_df)\n",
    "        y_arr = ex_y_df.to_numpy()\n",
    "\n",
    "        # check model inputs and outputs quantity\n",
    "        if len(model.inputs) > 2:\n",
    "            raise ValueError('There are no implementation for handling model with 3 inputs')\n",
    "        if len(model.outputs) > 2:\n",
    "            raise ValueError('There are no implementation for handling model with 3 inputs')\n",
    "\n",
    "        # if model has one or two inputs prepare data differently\n",
    "        x_crops_gen = None\n",
    "        if len(model.inputs) == 1:\n",
    "            x_arr = np.concatenate([dw.normalize_data(x_arr[:,:,:32]), dw.normalize_data(x_arr[:,:,32:])],axis=2)\n",
    "            x_crops_gen = dw.get_batch_generator(dw.get_crop_generator(x_arr, crop_size, crop_step), buffer)\n",
    "        else:\n",
    "            x_arr = np.concatenate([dw.standardize_data(x_arr[:,:,:32]), dw.standardize_data(x_arr[:,:,32:])],axis=2)\n",
    "            x_crops_gen = (dw.get_batch_generator(dw.get_crop_generator(x_arr[:,:,:32], crop_size, crop_step), buffer),\n",
    "                           dw.get_batch_generator(dw.get_crop_generator(x_arr[:,:,32:], crop_size, crop_step), buffer))\n",
    "\n",
    "        # if model has one or two outputs get predicts differently\n",
    "        res = list()\n",
    "        if len(load_model.outputs) == 1:\n",
    "            for x in x_crops_gen:\n",
    "                res.append(model.predict(x)[:,0])\n",
    "            res = np.concatenate(res)\n",
    "        else:\n",
    "            for time, amp in zip(*x_crops_gen):\n",
    "                pred = model.predict([time, amp])\n",
    "                bools = np.squeeze(pred[0], axis=1)\n",
    "                #depths = np.squeeze(pred[1], axis=1)\n",
    "                res.append(bools)\n",
    "            res = np.concatenate(res)\n",
    "\n",
    "        build = build_pred_map(ex_x_df, orig_shape, res, \n",
    "                    crop_size, crop_step, xshift=xshift)\n",
    "        \n",
    "        yield y_df, ex_y_df, build''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432fcc8-0190-4d21-baa8-a42c6fcb83b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''def make_pred_fast_new(model, \n",
    "                   path_to_data_tuple: tuple[str,str,str],\n",
    "                   crop_step_xshifts,\n",
    "                   **kvargs):\n",
    "    \"\"\"\n",
    "    Build prediction maps for the model. For given crop steps and\n",
    "    xshifts (Which are stored in crop_step_xshifts param as dict with\n",
    "    keys - crop steps values, and values - any iterable with xshifts for\n",
    "    the crop step).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tensorflow.keras.Model\n",
    "        The tensorflow model to use in prediction.\n",
    "    path_to_data_tuple: tuple[str,str,str]\n",
    "        The tuple with the paths for data file like \"*_data.csv\", \n",
    "        like \"*_defects.csv\" and like \"*_pipe.csv\".\n",
    "    **kvargs\n",
    "        Any additional params that may be defined in the get_xshifts_data_generator().\n",
    "    \"\"\"\n",
    "    x_df, y_df = dataset.get_x_and_y_data_dfs(dw.DataPart(path=path_to_data_tuple))\n",
    "    for crop_step, xshifts in crop_step_xshifts.items():\n",
    "        yield get_xshifts_data_generator(model, x_df, y_df, xshifts, crop_step, **kvargs)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e61925-471c-4a9d-9a3e-17caff4e25f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''crop_size_xshifts = {crop_step:list(range(XSHIFT,XSHIFT+crop_step,1)) \n",
    "                     for crop_step in CROP_STEPS}\n",
    "# print([type(item) for item in g])\n",
    "for run in RUNS:\n",
    "    print(f'run={run}')\n",
    "    data_gen = make_pred_fast_new(load_model, PATH_TO_DATA[f'run_{run}'], crop_size_xshifts)\n",
    "    for crop_step_xshift_gen in data_gen: # \n",
    "        for y_df, ex_y_df, res_gen in crop_step_xshift_gen:\n",
    "            for (orig_size_result_df, extend_size_result_df, info) in res_gen:\n",
    "                print(info, f'{orig_size_result_df.shape=}', f'{extend_size_result_df.shape=}')''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99dba5-1453-4de9-a4f9-5015bef3d30f",
   "metadata": {},
   "source": [
    "# Старый алгоритм построения предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194db4d-2777-4eff-80b9-cd61937abe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_fast(model, \n",
    "                   path_to_data_tuple: tuple(),\n",
    "                   x_shifts,\n",
    "                   y_shifts,\n",
    "                   crop_size: int, \n",
    "                   crop_steps): #, thold: float):\n",
    "\n",
    "    read_x_df, read_y_df = dataset.get_x_and_y_data_dfs(dw.DataPart(path=path_to_data_tuple))\n",
    "    \n",
    "    for x_shift in x_shifts:\n",
    "        for y_shift in y_shifts:\n",
    "            x_df = dw.roll_df(read_x_df, x_shift, 1)\n",
    "            y_df = dw.roll_df(read_y_df, x_shift, 1)\n",
    "        \n",
    "            #or_rows = x_df.shape[0]\n",
    "            #or_cols = x_df.shape[1]\n",
    "            \n",
    "            #ex_x_df = dw.extend_df_for_prediction(x_df, crop_size)\n",
    "            #ex_x_df = dw.match_df_for_crops_dividing(ex_x_df, crop_size, crop_step)\n",
    "            #ex_y_df = dw.extend_df_for_prediction(y_df, crop_size)\n",
    "            #ex_y_df = dw.match_df_for_crops_dividing(ex_y_df, crop_size, crop_step)\n",
    "            '''ex_x_df = dw.match_df_for_crops_dividing(x_df, crop_size, crop_step)\n",
    "            ex_y_df = dw.match_df_for_crops_dividing(y_df, crop_size, crop_step)'''\n",
    "    \n",
    "            \n",
    "            #ex_rows = ex_x_df.shape[0]\n",
    "            #ex_cols = ex_x_df.shape[1]\n",
    "    \n",
    "            #x_df = x_df.iloc[40+y_shift:,:] #40 0\n",
    "            #y_df = y_df.iloc[40+y_shift:,:]\n",
    "            x_df = x_df.iloc[y_shift:,:]\n",
    "            y_df = y_df.iloc[y_shift:,:]\n",
    "            print(f'xshift={x_shift}, yshift={y_shift}. Shape before: {y_df.shape}')\n",
    "            if x_df.shape[0]%crop_size != 0:\n",
    "                x_df = x_df.iloc[:-1*(x_df.shape[0]%crop_size)]\n",
    "                y_df = y_df.iloc[:-1*(y_df.shape[0]%crop_size)]\n",
    "            print(f'shape after: {y_df.shape}')\n",
    "            print()\n",
    "            x_arr = dw.df_to_numpy(x_df)\n",
    "            y_arr = y_df.to_numpy()\n",
    "    \n",
    "            #x_arr = x_arr[41:, :]\n",
    "            #y_arr = y_arr[41:, :]\n",
    "    \n",
    "            #x_arr = x_arr[:-1*(x_arr.shape[0]%crop_size)]\n",
    "            #y_arr = y_arr[:-1*(y_arr.shape[0]%crop_size)]\n",
    "        \n",
    "            # normalize_data\n",
    "            # standardize_datav\n",
    "            \n",
    "            x_arr = np.concatenate([dw.normalize_data(x_arr[:,:,:32]), dw.normalize_data(x_arr[:,:,32:])],axis=2)\n",
    "            x_crops_gen = dw.get_batch_generator(dw.get_crop_generator(x_arr, crop_size, crop_step), 15000)\n",
    "        \n",
    "            # 1 output\n",
    "            res = list()\n",
    "            for x in x_crops_gen:\n",
    "                #x = np.pad(x, ((0,0),(1,1),(1,1),(0,0)), 'reflect')\n",
    "                res.append(model.predict(x)[:,0])\n",
    "                #print(model.predict(x)[:,0].shape)\n",
    "            res = np.concatenate(res)\n",
    "            #print(res.shape)\n",
    "            res_it = iter(res)\n",
    "        \n",
    "            # 2 outputs\n",
    "            '''res = np.array(model.predict([x_data_time, x_data_amp]))\n",
    "            res = np.squeeze(res, axis=2)\n",
    "            bool_res = res[0,:]\n",
    "            depth_res = res[1,:] \n",
    "            bool_res_it = iter(bool_res) \n",
    "            depth_res_it = iter(depth_res) \n",
    "            res_it = bool_res_it'''\n",
    "            \n",
    "            def fill_by_arrays(df_cell_value):\n",
    "                return list()\n",
    "            def add_number_to_arrays(df_cell_value, number):\n",
    "                df_cell_value.append(number)\n",
    "                return df_cell_value\n",
    "                \n",
    "            extend_size_result_df = pd.DataFrame(data=0.0, \n",
    "                                     columns=x_df.columns, \n",
    "                                     index=x_df.index)\n",
    "            \n",
    "            #extend_size_result_df = extend_size_result_df.map(fill_by_arrays)\n",
    "            ex_rows = x_arr.shape[0]\n",
    "            ex_cols = x_arr.shape[1]\n",
    "            for i in range(0, ex_rows - crop_size + 1, crop_step):\n",
    "                for j in range(0,  ex_cols - crop_size + 1, crop_step):\n",
    "                    temp_add = next(res_it)\n",
    "                    #print(temp_add)\n",
    "                    #extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "                    #   extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: add_number_to_arrays(x, temp_add))\n",
    "                    #extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = temp_add\n",
    "                    '''if temp_add >= thold:\n",
    "                        extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "                            extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: x+1)\n",
    "                    else:\n",
    "                        extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = \\\n",
    "                            extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size].map(lambda x: x-1)'''\n",
    "                    \n",
    "                    extend_size_result_df.iloc[i:i+crop_size, j:j+crop_size] = temp_add\n",
    "        \n",
    "            '''extend_size_result_df = pd.DataFrame(data=dw.normalize_data(extend_size_result_df.to_numpy()), \n",
    "                                                 index=extend_size_result_df.index, \n",
    "                                                 columns=extend_size_result_df.columns)'''\n",
    "                                                 \n",
    "            #orig_size_result_df = extend_size_result_df.iloc[crop_size-1:, crop_size-1:].iloc[:or_rows, :or_cols]\n",
    "            #orig_size_result_df = extend_size_result_df.iloc[crop_size:, crop_size:].iloc[:or_rows-1, :or_cols-1]\n",
    "            #orig_size_result_df = extend_size_result_df.iloc[:or_rows, :or_cols]\n",
    "    \n",
    "            \n",
    "            yield extend_size_result_df, y_df, x_shift, y_shift #orig_size_result_df, extend_size_result_df, y_df, ex_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d38f5-4059-4915-89e4-a2ea2ac788c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''arr = np.arange(11)\n",
    "print(arr)\n",
    "quart = [*np.quantile(arr,[0.25,0.5,0.75]),arr.max()]\n",
    "print(quart)\n",
    "\n",
    "# calc hist of elemens of quartiles\n",
    "def hist(arr: np.ndarray, bounds: list[int], res: list[int]=[]):\n",
    "    if not bounds or not arr.shape:\n",
    "        return res\n",
    "    bound = bounds.pop(0)\n",
    "    res.append(arr[arr<=bound].size)\n",
    "    return hist(arr[arr>bound], bounds, res)\n",
    "\n",
    "print('hists: ', hist(arr, quart))''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9103895-3237-45e4-b8ea-ef51c63e5fa6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_expected_value(df_cell):\n",
    "    arr = np.array(df_cell)\n",
    "    uniques, counts = np.unique(arr, return_counts=True)\n",
    "    weights = counts/arr.size\n",
    "    return (uniques * weights).sum() / weights.sum()\n",
    "\n",
    "def calc_1_quartile(df_cell):\n",
    "    return np.quantile(df_cell,[0.25])\n",
    "def calc_2_quartile(df_cell):\n",
    "    return np.quantile(df_cell,[0.5])\n",
    "def calc_3_quartile(df_cell):\n",
    "    return np.quantile(df_cell,[0.75])\n",
    "'''def calc_half(df_cell):\n",
    "    return np.quantile(df_cell,[0.5])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc6ea9-3f1e-44a6-8a24-7d05b95722ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# new\n",
    "for run in RUNS:\n",
    "    for crop_step in CROP_STEPS:\n",
    "        xshifts_range = range(XSHIFT,XSHIFT+crop_step,1)\n",
    "        data_gen = make_pred_fast(load_model, PATH_TO_DATA[f'run_{run}'], xshifts_range, CROP_SIZE, CROP_STEPS)\n",
    "        for xshift in xshifts_range:\n",
    "            #logger.debug(f'run: {run}, crop step: {crop_step}')\n",
    "            (orig_size_result_df, \n",
    "            extend_size_result_df,\n",
    "            orig_size_reference_df, \n",
    "            extend_size_reference_df) = next(data_gen)\n",
    "            \n",
    "            p_data = 3 # if crop_step >= 8 else 2\n",
    "\n",
    "            '''bins_arr = [101] #6,11,21,\n",
    "            if crop_step < 8:\n",
    "                bind_arr = bins_arr[:3]'''\n",
    "            \n",
    "            for bins in [21]: #\n",
    "                path_to_file = (pathlib.Path(f'images/models predicts')/\n",
    "                                f'model_id=v{MODEL_VER:02}n{MODEL_NUM:02}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)_epoch={EPOCH}'/\n",
    "                                f'run_{run}'/'xshifts'/f'crop_step={crop_step}')\n",
    "                '''if not os.path.exists(path_to_file):\n",
    "                    os.makedirs(path_to_file)\n",
    "                dw.draw_defects_map(orig_size_reference_df, \n",
    "                    title = f'REFERENCE run = {run}',\n",
    "                    path_to_save = str(path_to_file/f'REFERENCE run={run}.jpg'))'''\n",
    "\n",
    "                #bins = 21 if crop_step == 4 else 101\n",
    "                \n",
    "                #for func in [max, min, np.mean, calc_half, calc_quartile, calc_expected_value]:\n",
    "                for func in [calc_1_quartile, calc_2_quartile, calc_3_quartile]: #[max, min, np.mean, calc_1_quartile, calc_2_quartile, calc_3_quartile, calc_expected_value]:\n",
    "                \n",
    "                    '''func_path_to_file = None\n",
    "\n",
    "                    if len(xshifts_range) == 1:\n",
    "                        func_path_to_file = path_to_file\n",
    "                    elif len(xshifts_range) == 2:\n",
    "                        func_path_to_file = path_to_file/func.__name__\n",
    "                    else:\n",
    "                        func_path_to_file = path_to_file/func.__name__/f'bins={bins}'''\n",
    "\n",
    "                    func_path_to_file = path_to_file/func.__name__\n",
    "                    \n",
    "                    if not os.path.exists(func_path_to_file):\n",
    "                        os.makedirs(func_path_to_file)\n",
    "        \n",
    "                    or_temp = orig_size_result_df.map(lambda x: func(x))\n",
    "\n",
    "                    dw.draw_defects_map_with_reference_owerlap(or_temp, orig_size_reference_df, \n",
    "                        title = f'OVERLAPPED PREDICTION (xshift={xshift}, bins={bins}, func={func.__name__}, run={run}, crop step={crop_step}, crop size={CROP_SIZE})',\n",
    "                        polygonize_data=p_data, bins=bins,\n",
    "                        path_to_save = func_path_to_file/f'OVERLAPPED PREDICTION (xshift={xshift}, bins={bins}, func={func.__name__}, run={run}, crop step={crop_step}, crop size={CROP_SIZE}).png')\n",
    "\n",
    "                    '''dw.draw_defects_map(or_temp, \n",
    "                        title = f'PREDICTION {func.__name__} run: {run}, crop step = {crop_step}, crop size = {CROP_SIZE}',\n",
    "                        path_to_save = func_path_to_file/f'PREDICTION {func.__name__} run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg')'''\n",
    "        \n",
    "                    \n",
    "                    '''binary = np.arange(0.1,1,0.1)\n",
    "        \n",
    "                    for bina in binary:\n",
    "                        bin_func_path_to_file = func_path_to_file/f'binary_{bina:0.2f}'\n",
    "                    \n",
    "                        if not os.path.exists(bin_func_path_to_file):\n",
    "                            os.makedirs(bin_func_path_to_file)\n",
    "                    \n",
    "                        temp = or_temp.map(lambda x: 1 if x >= bina else 0) # or_temp\n",
    "                        \n",
    "                        dw.draw_defects_map(temp, \n",
    "                            title = f'PREDICTION {func.__name__} binary_{bina:.02f} run = {run}, crop step = {crop_step}, crop size = {CROP_SIZE}',\n",
    "                            path_to_save = str(bin_func_path_to_file/\n",
    "                                f'PREDICTION {func.__name__} binary_{bina:.02f} run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))\n",
    "                    \n",
    "                        #bin_func_path_to_file = bin_func_path_to_file/f'OVERLAPPED' #f'{func_path_to_file}/binary_{bina}'\n",
    "                    \n",
    "                        #if not os.path.exists(bin_func_path_to_file):\n",
    "                        #    os.makedirs(bin_func_path_to_file)\n",
    "                        \n",
    "                        dw.draw_defects_map_with_reference_owerlap(temp, orig_size_reference_df, \n",
    "                            title = f'OVERLAPPED PREDICTION {func.__name__} binary_{bina:.02f} run = {run},'+\n",
    "                               f' crop step = {crop_step}, crop size = {CROP_SIZE}',\n",
    "                            path_to_save = str(bin_func_path_to_file/\n",
    "                               f'OVERLAPPED PREDICTION {func.__name__} binary_{bina:.02f} run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))''';\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110854f0-be71-467d-8f3b-aa47f972b523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for (thold, name) in [(0.1, 'best_recall'), (0.28, 'best_mean_values'), (0.4,'best_precision')]:\n",
    "for run in RUNS:\n",
    "    for crop_step in CROP_STEPS:\n",
    "        xshifts_range = range(XSHIFT,XSHIFT+crop_step,1)\n",
    "        yshifts_range = range(8) #15 8 4\n",
    "        data_gen = make_pred_fast(load_model, PATH_TO_DATA[f'run_{run}'], xshifts_range, yshifts_range, CROP_SIZE, CROP_STEPS)\n",
    "        for (orig_size_result_df, orig_size_reference_df, xshift, yshift) in data_gen:\n",
    "            #logger.debug(f'run: {run}, crop step: {crop_step}')\n",
    "            #orig_size_result_df, orig_size_reference_df, x_shift, y_shift = next(data_gen)\n",
    "            \n",
    "            #bins_arr = [101, 1001]\n",
    "\n",
    "            #for bins in bins_arr:\n",
    "            path_to_file = (pathlib.Path(f'data/generated_content/CNN/in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)/')/\n",
    "                            f'model_id=v{MODEL_VER:04}n{MODEL_NUM:04}_epoch={EPOCH}'/\n",
    "                            'test+val'/f'crop_step={crop_step}') # /f'bins={bins}' /'xshifts' f'run_{run}'\n",
    "            \n",
    "            if not os.path.exists(path_to_file):\n",
    "                os.makedirs(path_to_file)\n",
    "            # bins=bins, \n",
    "            dw.draw_defects_map_with_reference_owerlap(orig_size_result_df, orig_size_reference_df, \n",
    "                title = f'OVERLAPPED PREDICTION (xshift={xshift}, yshift={yshift}, run={run}, crop step={crop_step}, crop size={CROP_SIZE})',\n",
    "                polygonize_data=3, \n",
    "                path_to_save = path_to_file/f'OVERLAPPED PREDICTION (xshift={xshift}, yshift={yshift}, run={run}, crop step={crop_step}, crop size={CROP_SIZE}).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c78df-1bec-4d45-8ea6-d9906fd23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''arr = np.arange(34)\n",
    "print(arr)\n",
    "print(arr.shape[0]%16)\n",
    "print(arr[:-1*(arr.shape[0]%16)])\n",
    "23%16'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b170e1-58fe-49a1-a7f3-2762ce1034a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# old\n",
    "#for (thold, name) in [(0.1, 'best_recall'), (0.28, 'best_mean_values'), (0.4,'best_precision')]: #np.arange(0.1,1,0.1):\n",
    "for xshift in range(200,217,1):\n",
    "    for run in RUNS:\n",
    "        for crop_step in CROP_STEPS:\n",
    "            #logger.debug(f'thold: {thold}, run: {run}, crop step: {crop_step}')\n",
    "            (orig_size_result_df, \n",
    "            extend_size_result_df,\n",
    "            orig_size_reference_df, \n",
    "            extend_size_reference_df) = make_pred_fast(load_model, PATH_TO_DATA[f'run_{run}'], xshift, CROP_SIZE, crop_step)#, thold)\n",
    "    \n",
    "            path_to_file = (pathlib.Path('images/models predicts')/\n",
    "                            f'model_id=v{MODEL_VER:02}n{MODEL_NUM:02}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)_epoch={EPOCH}'/\n",
    "                            f'run_{run}'/f'xhifts') \n",
    "    \n",
    "            if not os.path.exists(path_to_file):\n",
    "                os.makedirs(path_to_file)\n",
    "            \n",
    "            dw.draw_defects_map(orig_size_reference_df, \n",
    "                title = f'REFERENCE run = {run}, xhift={xshift}',\n",
    "                path_to_save = str(path_to_file/f'xhift={xshift}_REFERENCE run={run}.jpg'))\n",
    "            \n",
    "            #path_to_file = path_to_file/'old_math_reduce(add_or_remove=1)'/f'trashhold={thold:0.1f}({name})'\n",
    "            #path_to_file = path_to_file/'old_math_reduce(add_model_pred)'\n",
    "            \n",
    "            #if not os.path.exists(path_to_file):\n",
    "            #    os.makedirs(path_to_file)\n",
    "            \n",
    "            dw.draw_defects_map(orig_size_result_df, \n",
    "                title = f'PREDICTION run: {run}, crop step = {crop_step}, crop size = {CROP_SIZE}, xhift={xshift}')#,\n",
    "                #path_to_save = str(path_to_file/f'xhift={xshift}_PREDICTION run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))'''\n",
    "                \n",
    "            dw.draw_defects_map_with_reference_owerlap(orig_size_result_df, orig_size_reference_df, \n",
    "                title = f'OVERLAPPED PREDICTION run: {run}, crop step = {crop_step}, crop size = {CROP_SIZE}, xhift={xshift}', \n",
    "                annotate_data=True, annotate_font_size=crop_step)#,\n",
    "                #path_to_save = str(path_to_file/f'xhift={xshift}_OVERLAPPED PREDICTION run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))\n",
    "    \n",
    "            binary = np.arange(0.1,1,0.1)\n",
    "    \n",
    "            for bina in binary:\n",
    "                bin_func_path_to_file = path_to_file/f'binary_{bina:0.2f}'\n",
    "            \n",
    "                if not os.path.exists(bin_func_path_to_file):\n",
    "                    os.makedirs(bin_func_path_to_file)\n",
    "    \n",
    "                temp = orig_size_result_df.map(lambda x: 1 if x >= bina else 0) # or_temp\n",
    "                \n",
    "                dw.draw_defects_map(temp, \n",
    "                    title = f'PREDICTION binary_{bina:.02f} run = {run}, crop step = {crop_step}, crop size = {CROP_SIZE}',\n",
    "                    path_to_save = str(bin_func_path_to_file/\n",
    "                                f'PREDICTION binary_{bina:.02f} run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))\n",
    "    \n",
    "                #bin_func_path_to_file = bin_func_path_to_file/f'OVERLAPPED' #f'{func_path_to_file}/binary_{bina}'\n",
    "            \n",
    "                #if not os.path.exists(bin_func_path_to_file):\n",
    "                #    os.makedirs(bin_func_path_to_file)\n",
    "                \n",
    "                dw.draw_defects_map_with_reference_owerlap(temp, orig_size_reference_df, \n",
    "                    title = f'OVERLAPPED PREDICTION binary_{bina:.02f} run = {run}, crop step = {crop_step}, crop size = {CROP_SIZE}',\n",
    "                    path_to_save = str(bin_func_path_to_file/\n",
    "                                   f'OVERLAPPED PREDICTION binary_{bina:.02f} run={run}, crop step={crop_step}, crop size={CROP_SIZE}.jpg'))\"\"\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcc8322-9ef3-4f86-8fcb-e98e541c6091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:08:49.760096Z",
     "iopub.status.busy": "2024-03-08T12:08:49.760096Z",
     "iopub.status.idle": "2024-03-08T12:08:53.580369Z",
     "shell.execute_reply": "2024-03-08T12:08:53.580369Z",
     "shell.execute_reply.started": "2024-03-08T12:08:49.760096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Narchi\\anaconda3\\envs\\network\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dependencies import\n",
    "from common_dependencies import *\n",
    "\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "from shapely.geometry import Polygon as shPolygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from docx.enum.section import WD_ORIENT\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Cm\n",
    "from docx.shared import Pt\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456d93ae-e033-4ee0-a8cd-f52879c23640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:08:53.581369Z",
     "iopub.status.busy": "2024-03-08T12:08:53.581369Z",
     "iopub.status.idle": "2024-03-08T12:08:53.596372Z",
     "shell.execute_reply": "2024-03-08T12:08:53.596372Z",
     "shell.execute_reply.started": "2024-03-08T12:08:53.581369Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_GET_DATA = 'data\\\\drawing_data'\n",
    "PATH_TO_SAVE_DATA = 'data\\\\report'\n",
    "MODEL_VERS = [] # для каких версий моделей делать отчет\n",
    "MODEL_NUMS = [] # для каких номеров моделей делать отчет\n",
    "\n",
    "model_runs = r'\\d{2}' if len(MODEL_VERS) == 0 else f\"({'|'.join([model_ver for model_ver in MODEL_VERS])})\"\n",
    "model_nums = r'\\d{2}' if len(MODEL_NUMS) == 0 else f\"({'|'.join([model_num for model_num in MODEL_NUMS])})\"\n",
    "# пути к каким версиям и номерам моделей учитывать при создании отчетов\n",
    "model_folder_regex = r'.*\\\\model_id=' + f'v{model_runs}n{model_nums}$'\n",
    "run_folder_regex = model_folder_regex[:-1] + r'\\\\run_\\d+$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a939c881-0731-4b3e-84ec-0e694dab4dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:08:53.597373Z",
     "iopub.status.busy": "2024-03-08T12:08:53.597373Z",
     "iopub.status.idle": "2024-03-08T12:08:53.612376Z",
     "shell.execute_reply": "2024-03-08T12:08:53.612376Z",
     "shell.execute_reply.started": "2024-03-08T12:08:53.597373Z"
    }
   },
   "outputs": [],
   "source": [
    "def decrease_blur(df_cell_value):\n",
    "    res = 2*df_cell_value**2 if df_cell_value <= 0.5 else 1-(2*(1-df_cell_value)**2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb848a4e-6108-457e-bb1f-092b21f9c8ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:08:53.614376Z",
     "iopub.status.busy": "2024-03-08T12:08:53.613377Z",
     "iopub.status.idle": "2024-03-08T12:12:20.938896Z",
     "shell.execute_reply": "2024-03-08T12:12:20.938896Z",
     "shell.execute_reply.started": "2024-03-08T12:08:53.614376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working path:  data\\drawing_data\\model_id=v02n01\\run_1\n",
      "model_id=v02n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n01\\run_2\n",
      "model_id=v02n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n02\\run_1\n",
      "model_id=v02n02_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n02\\run_2\n",
      "model_id=v02n02_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n02_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n03\\run_1\n",
      "model_id=v02n03_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n03\\run_2\n",
      "model_id=v02n03_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n03_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n04\\run_1\n",
      "model_id=v02n04_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v02n04\\run_2\n",
      "model_id=v02n04_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v02n04_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n01\\run_1\n",
      "model_id=v03n01_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n01\\run_2\n",
      "model_id=v03n01_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n01_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n02\\run_1\n",
      "model_id=v03n02_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n02\\run_2\n",
      "model_id=v03n02_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n02_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n03\\run_1\n",
      "model_id=v03n03_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v03n03\\run_2\n",
      "model_id=v03n03_crop(size=8,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v03n03_crop(size=8,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n01\\run_1\n",
      "model_id=v04n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n01\\run_2\n",
      "model_id=v04n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n02\\run_1\n",
      "model_id=v04n02_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n02\\run_2\n",
      "model_id=v04n02_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n02_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n03\\run_1\n",
      "model_id=v04n03_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n03\\run_2\n",
      "model_id=v04n03_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n03_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n04\\run_1\n",
      "model_id=v04n04_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n04\\run_2\n",
      "model_id=v04n04_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n04_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n05\\run_1\n",
      "model_id=v04n05_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v04n05\\run_2\n",
      "model_id=v04n05_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v04n05_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v05n01\\run_1\n",
      "model_id=v05n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v05n01\\run_2\n",
      "model_id=v05n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v05n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v06n01\\run_1\n",
      "model_id=v06n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n",
      "Current working path:  data\\drawing_data\\model_id=v06n01\\run_2\n",
      "model_id=v06n01_crop(size=16,step=1)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=16)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=2)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=4)_shift(x=200,y=0).xlsx  processing...\n",
      "model_id=v06n01_crop(size=16,step=8)_shift(x=200,y=0).xlsx  processing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create temp folder \"report\" in 'PATH_TO_DATA' parent folder\n",
    "# with identical folder tree structure as 'PATH_TO_DATA'\n",
    "# and fill every folder with jpgs instead of .xlsx files\n",
    "\n",
    "# BLUR ONES\n",
    "\n",
    "for run_path in os.walk(PATH_TO_GET_DATA):\n",
    "    ref = True\n",
    "    if re.match(run_folder_regex, run_path[0]) is None:\n",
    "        continue\n",
    "\n",
    "    full_path_to_save_run_files = run_path[0].replace(PATH_TO_GET_DATA, PATH_TO_SAVE_DATA)\n",
    "    \n",
    "    if not os.path.exists(full_path_to_save_run_files):\n",
    "        os.makedirs(full_path_to_save_run_files)\n",
    "    \n",
    "    print('Current working path: ',run_path[0])\n",
    "    for file_name in run_path[2]:\n",
    "        if re.match('.*\\.xlsx', file_name) is None:\n",
    "            continue\n",
    "        print(file_name, ' processing...')\n",
    "        \n",
    "        if ref == True:\n",
    "            ref_df = pd.read_excel(os.path.join(run_path[0], file_name),\n",
    "                         sheet_name='orig_reference', index_col=0, dtype=np.float64)\n",
    "            dw.draw_defects_map(ref_df, \n",
    "                title = 'REFERENCE ' + file_name,\n",
    "                path_to_save = full_path_to_save_run_files + '\\\\' + 'REFERENCE ' + file_name + '.jpg')\n",
    "            ref = False\n",
    "            \n",
    "        pred_df = pd.read_excel(os.path.join(run_path[0], file_name),\n",
    "                      sheet_name='orig_res', index_col=0, dtype=np.float64)\n",
    "\n",
    "        df_values = pred_df.to_numpy()\n",
    "        df_indexes = pred_df.index.to_numpy()\n",
    "        df_columns = pred_df.columns.to_numpy()\n",
    "\n",
    "        df_values =  (df_values - df_values.min()) / (df_values.max() - df_values.min())\n",
    "\n",
    "        pred_df = pd.DataFrame(data=df_values,index=df_indexes,columns=df_columns)\n",
    "        \n",
    "        dw.draw_defects_map(pred_df, \n",
    "            title = 'PREDICTION ' + file_name,\n",
    "            path_to_save = full_path_to_save_run_files + '\\\\' + 'PREDICTION ' + file_name + '.jpg')\n",
    "        \n",
    "        dw.draw_defects_map_with_reference_owerlap(pred_df, ref_df, \n",
    "            title = 'OVERLAPPED PREDICTION ' + file_name,\n",
    "            path_to_save = full_path_to_save_run_files + '\\\\' + 'OVERLAPPED PREDICTION ' + file_name + '.jpg')\n",
    "\n",
    "        pred_df1 = pred_df.map(decrease_blur)\n",
    "\n",
    "        dw.draw_defects_map(pred_df1, \n",
    "            title = 'UNBLUR PREDICTION ' + file_name,\n",
    "            path_to_save = full_path_to_save_run_files + '\\\\' + 'UNBLUR PREDICTION ' + file_name + '.jpg')\n",
    "        \n",
    "        dw.draw_defects_map_with_reference_owerlap(pred_df1, ref_df, \n",
    "            title = 'UNBLUR OVERLAPPED PREDICTION ' + file_name,\n",
    "            path_to_save = full_path_to_save_run_files + '\\\\' + 'UNBLUR OVERLAPPED PREDICTION ' + file_name + '.jpg')\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee2c1f2c-8a9e-4909-bc9c-4688fe855466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:12:20.939897Z",
     "iopub.status.busy": "2024-03-08T12:12:20.939897Z",
     "iopub.status.idle": "2024-03-08T12:12:20.970152Z",
     "shell.execute_reply": "2024-03-08T12:12:20.970152Z",
     "shell.execute_reply.started": "2024-03-08T12:12:20.939897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImageDescription = NamedTuple(\"ImageDescription\", \n",
    "                                  [('model_id', str), \n",
    "                                   ('run_num', int),\n",
    "                                   ('image_type_name', str),\n",
    "                                   ('crop_step', int),\n",
    "                                   ('path', str)])\n",
    "image_descriptions_list = []\n",
    "\n",
    "\n",
    "for run_path in os.walk(PATH_TO_SAVE_DATA):\n",
    "    if re.search(run_folder_regex, run_path[0]) is None:\n",
    "        continue\n",
    "    model_id = re.findall(r'model_id=(v\\d{2}n\\d{2})', run_path[0])[0]\n",
    "    run_num = re.findall(r'run_(\\d+)', run_path[0])[0]\n",
    "    for image_name in os.listdir(run_path[0]):\n",
    "        if re.match('.*\\.jpg', image_name) is None:\n",
    "            continue\n",
    "\n",
    "        crop_step = re.findall(r'step=(\\d+)', image_name)[0]\n",
    "        image_type_name = re.search(r'[A-Z\\s]+', image_name)[0].strip()\n",
    "        image_descriptions_list.append(ImageDescription(model_id, \n",
    "                                                        int(run_num), \n",
    "                                                        image_type_name, \n",
    "                                                        int(crop_step),\n",
    "                                                        os.path.join(run_path[0], image_name)))\n",
    "        #print(os.path.join(run_path[0], image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab90ebf-1a3e-4578-97ef-43af249ccd36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T12:12:20.971154Z",
     "iopub.status.busy": "2024-03-08T12:12:20.971154Z",
     "iopub.status.idle": "2024-03-08T12:12:22.810541Z",
     "shell.execute_reply": "2024-03-08T12:12:22.810541Z",
     "shell.execute_reply.started": "2024-03-08T12:12:20.971154Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 8\n",
    "for model_path in os.walk(PATH_TO_SAVE_DATA):\n",
    "    if re.match(model_folder_regex, model_path[0]) is None:\n",
    "        continue\n",
    "    model_id = re.findall(r'model_id=(v\\d{2}n\\d{2})', model_path[0])[0]\n",
    "    # images related to current model\n",
    "    model_images = [image_desc for image_desc in image_descriptions_list if image_desc.model_id == model_id]\n",
    "    # numbers of \"run_\"\n",
    "    run_numbers = list(set([image_desc.run_num for image_desc in model_images]))\n",
    "    run_numbers.sort()\n",
    "    \n",
    "    document = Document()\n",
    "    document.add_heading(f'MODEL_ID={model_id} PREDICTION RESULTS', 0)\n",
    "\n",
    "    for run in run_numbers:\n",
    "        ref_image = [image_desc for image_desc in model_images \n",
    "                     if image_desc.run_num == run and image_desc.image_type_name == 'REFERENCE']\n",
    "\n",
    "        \n",
    "        pred_images = [image_desc for image_desc in model_images \n",
    "                     if image_desc.run_num == run and image_desc.image_type_name == 'PREDICTION']\n",
    "        pred_images.sort(key=lambda x: x.crop_step, reverse=True)\n",
    "\n",
    "        unblur_pred_images = [image_desc for image_desc in model_images \n",
    "                 if image_desc.run_num == run and image_desc.image_type_name == 'UNBLUR PREDICTION']\n",
    "        unblur_pred_images.sort(key=lambda x: x.crop_step, reverse=True)\n",
    "\n",
    "        \n",
    "        overlapped_pred_images = [image_desc for image_desc in model_images \n",
    "                     if image_desc.run_num == run and image_desc.image_type_name == 'OVERLAPPED PREDICTION']\n",
    "        overlapped_pred_images.sort(key=lambda x: x.crop_step, reverse=True)\n",
    "\n",
    "        unblur_overlapped_pred_images = [image_desc for image_desc in model_images \n",
    "                     if image_desc.run_num == run and image_desc.image_type_name == 'UNBLUR OVERLAPPED PREDICTION']\n",
    "        unblur_overlapped_pred_images.sort(key=lambda x: x.crop_step, reverse=True)\n",
    "    \n",
    "        \n",
    "        document.add_heading(f'RUN {run}', 1)\n",
    "        document.add_heading('REFERENCE', 2)\n",
    "        document.add_picture(ref_image[0].path, width=Inches(IMAGE_WIDTH))\n",
    "        \n",
    "        document.add_heading('MODEL PREDICTION', 2)\n",
    "        for image_desc in pred_images:\n",
    "            document.add_picture(image_desc.path, width=Inches(IMAGE_WIDTH))\n",
    "\n",
    "        document.add_heading('UNBLUR MODEL PREDICTION', 2)\n",
    "        for image_desc in unblur_pred_images:\n",
    "            document.add_picture(image_desc.path, width=Inches(IMAGE_WIDTH))\n",
    "        \n",
    "        document.add_heading('OVERLAPPED WITH REFERENCE MODEL PREDICTION', 2)\n",
    "        for image_desc in overlapped_pred_images:\n",
    "            document.add_picture(image_desc.path, width=Inches(IMAGE_WIDTH))\n",
    "\n",
    "        document.add_heading('UNBLUR OVERLAPPED WITH REFERENCE MODEL PREDICTION', 2)\n",
    "        for image_desc in unblur_overlapped_pred_images:\n",
    "            document.add_picture(image_desc.path, width=Inches(IMAGE_WIDTH))\n",
    "        \n",
    "        document.add_page_break()\n",
    "        \n",
    "    section = document.sections[-1]\n",
    "    \n",
    "    #new_width, new_height = section.page_height, section.page_width\n",
    "    #section.orientation = WD_ORIENT.LANDSCAPE\n",
    "    #section.page_width = new_width\n",
    "    #section.page_height = new_height\n",
    "    \n",
    "    section.top_margin = Cm(0.5)\n",
    "    section.bottom_margin = Cm(0.5)\n",
    "    section.left_margin = Cm(0.5)\n",
    "    section.right_margin = Cm(0.5)\n",
    "\n",
    "    head0_style = document.styles['Title']\n",
    "    head1_style = document.styles['Heading 1']\n",
    "    head2_style = document.styles['Heading 2']\n",
    "\n",
    "    head0_style.font.size = Pt(24)\n",
    "    head1_style.font.size = Pt(22)\n",
    "    head2_style.font.size = Pt(20)\n",
    "    \n",
    "    head0_style.font.color.rgb = RGBColor(255, 0, 0)\n",
    "    head1_style.font.color.rgb = RGBColor(255, 0, 0)\n",
    "    head2_style.font.color.rgb = RGBColor(255, 0, 0)\n",
    "    \n",
    "    document.save(os.path.join(model_path[0], 'report.docx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafa097-de10-4b62-b33e-ba651e42f1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde58157-7750-45b9-a343-b46e3c74b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies import\n",
    "from common_dependencies import *\n",
    "import model_versions as mv\n",
    "import time\n",
    "logger = logging.getLogger(f'main.model_train')\n",
    "# детерминация случайных величин, отвечающих за выбор первоначальных весов и биасов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf461c0-86ef-4815-a965-1e766c1f33b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train params that can be easily changed\n",
    "tf.compat.v1.set_random_seed(290)\n",
    "tf.random.set_seed(290)\n",
    "np.random.seed(290)\n",
    "\n",
    "CROP_SIZE = 16\n",
    "#CROP_STEP = 16\n",
    "MAX_VAL = 1000\n",
    "XSHIFT = 200\n",
    "MODEL_VER = '14'\n",
    "#OLD_MODEL_NUM = '03'\n",
    "NEW_MODEL_NUM = '05'\n",
    "PATH_TO_SAVE_CHECKPOINTS = f\"networks/CNN/checkpoints/model_id=v{MODEL_VER}n{NEW_MODEL_NUM}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)\"\n",
    "PATH_TO_SAVE_LOGS = f\"networks/CNN/logs/model_id=v{MODEL_VER}n{NEW_MODEL_NUM}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)\"\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_CHECKPOINTS):\n",
    "        os.makedirs(PATH_TO_SAVE_CHECKPOINTS)\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_LOGS):\n",
    "        os.makedirs(PATH_TO_SAVE_LOGS)\n",
    "    \n",
    "dataset_desc = {'train': (\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(0,0,140,MAX_VAL),SlidingCrop(CROP_SIZE,4),XSHIFT),\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(140,0,120,MAX_VAL),SlidingCrop(CROP_SIZE,1),XSHIFT),\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(260,0,MAX_VAL,MAX_VAL),SlidingCrop(CROP_SIZE,4),XSHIFT),\n",
    "                         ),\n",
    "\n",
    "                'val': (\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(195,0,85,MAX_VAL),SlidingCrop(CROP_SIZE,1),XSHIFT),\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(280,0,MAX_VAL,MAX_VAL),SlidingCrop(CROP_SIZE,4),XSHIFT),\n",
    "                \n",
    "                        ),\n",
    "                'test': (\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(0,0,110,MAX_VAL),SlidingCrop(CROP_SIZE,4),XSHIFT),\n",
    "                            DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(110,0,85,MAX_VAL),SlidingCrop(CROP_SIZE,1),XSHIFT),\n",
    "                \n",
    "                        )\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bb7c5-b83f-4694-b33f-11b1a30ef57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display info about the dataset used in model training\n",
    "\n",
    "dataset = {'train':dict(zip(['x','y','bin'], chain_dataset_gens(dataset_desc['train']))),\n",
    "           'val':dict(zip(['x','y','bin'], chain_dataset_gens(dataset_desc['val']))),\n",
    "           'test':dict(zip(['x','y','bin'], chain_dataset_gens(dataset_desc['test'])))}\n",
    "\n",
    "# shape displaying\n",
    "print('Dataset')\n",
    "for dataset_part_name, dataset_part in dataset.items():\n",
    "    print('|'*8+dataset_part_name+'|'*8)\n",
    "    for data_part_name, data_part in dataset_part.items():\n",
    "        item_shape = next(data_part).shape if type(next(data_part)) == np.ndarray else '(1)'\n",
    "        print(f'{data_part_name} items total: ({sum(1 for x in data_part)+1}). Item shape: {item_shape}')\n",
    "\n",
    "#выборка данных\n",
    "# show parts took for learning\n",
    "all_rects = {'run_1': {'train':None,'val':None,'test':None}, \n",
    "             'run_2': {'train':None,'val':None,'test':None}}\n",
    "rects_colors = {'train':'red', 'val':'green', 'test':'yellow'}\n",
    "\n",
    "for run_name in all_rects.keys():\n",
    "    x_df, y_df = dataset.get_x_and_y_data_dfs(dw.DataPart(path_to_run_folder=PATH_TO_DATA[run_name]))\n",
    "    x_df = None\n",
    "    y_df = dw.roll_df(y_df, XSHIFT, 1)\n",
    "    for dataset_part_name in all_rects[run_name].keys():\n",
    "        # get all DatasetPartDescription for train, val or test\n",
    "        dataset_part_desc = dataset_desc[dataset_part_name]\n",
    "        # get all DatasetPartDescription for current run_name (run_1 or run_2)\n",
    "        dataset_part_desc = [dataset_part for dataset_part in dataset_part_desc if re.findall(r'run_\\d', dataset_part.data_path_tuple[0])[0] == run_name]\n",
    "        # put rects list to all_rects[run_name][dataset_part_name]\n",
    "        all_rects[run_name][dataset_part_name] = [Rectangle((dataset_part.file_data_crop.left, dataset_part.file_data_crop.top), \n",
    "                           dataset_part.file_data_crop.width, dataset_part.file_data_crop.height, \n",
    "                           facecolor=rects_colors[dataset_part_name], alpha=0.4, edgecolor='white')\n",
    "                                                  for dataset_part in dataset_part_desc]\n",
    "        \n",
    "    res_rects = list(itertools.chain(*[run_rects for run_rects_name, run_rects in all_rects[run_name].items()]))\n",
    "    if res_rects:\n",
    "        dw.draw_defects_map_with_rectangles_owerlap(y_df, res_rects, title = f'The parts took for learning from {run_name} (red - train, green - validate, yellow - test)')\n",
    "    else:\n",
    "        dw.draw_defects_map(y_df, title = f'The parts took for learning from {run_name} (red - train, green - validate, yellow - test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00010013-0c7f-42cb-a798-466569c34725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### всякие константы для последующей работы\n",
    "\n",
    "#///////////////////////////////// для компиляции \n",
    "\n",
    "CMP_learning_rate = 0.0005 #0.0000002 # шаг сходимости back propogation\n",
    "#CMP_solver = keras.optimizers.Adam(CMP_learning_rate) # оптимизатор\n",
    "CMP_solver = keras.optimizers.SGD(CMP_learning_rate) # оптимизатор\n",
    "CMP_loss_func = keras.losses.BinaryCrossentropy() #BinaryCrossentropy() \n",
    "CMP_metrics = [keras.metrics.BinaryCrossentropy(name='loss'),\n",
    "               keras.metrics.BinaryAccuracy(name='BinaryAccuracy'),\n",
    "               keras.metrics.MeanSquaredError(name='MSE'),\n",
    "               #keras.metrics.TruePositives(name='TruePositives'),\n",
    "               #keras.metrics.FalsePositives(name='FalsePositives'),\n",
    "               #keras.metrics.TrueNegatives(name='TrueNegatives'),\n",
    "               #keras.metrics.FalseNegatives(name='FalseNegatives'),     \n",
    "               keras.metrics.Precision(name='Precision'),\n",
    "               keras.metrics.Recall(name='Recall'),\n",
    "               keras.metrics.AUC(name='PR_AUC', curve='PR')\n",
    "              ]\n",
    "#///////////////////////////////// для колбэков\n",
    "\n",
    "'''# для Early_stopping\n",
    "ES_patience = 6 # кол-во эпох без улучшений\n",
    "ES_min_delta = 0.00001 # минимальное улучшение параметра за cur_patience\n",
    "ES_monitor_parametr =  'loss' # отслеживаемый параметр \n",
    "ES_save_best_weights = False # сохранять ли веса нейронки с лучшими результатами\n",
    "Es_mode = 'max'\n",
    "\n",
    "   # для ReduceLROnPlateau\n",
    "RLPOP_monitor_parametr = 'loss'  # отслеживаемый параметр \n",
    "RLPOP_factor = 0.3 # множитель для расчета нового шага сходимости (new_learning_rate = old_learning_rate*RLPOP_factor)\n",
    "RLPOP_patience = 1 # кол-во эпох без улучшений\n",
    "RLPOP_verbose = 1 # выводить ли прогресс изменения шага сходимости в его процессее\n",
    "RLPOP_mode = 'auto' # выбирает, уменьшать шаг сходимости при росте величины или при её уменьшении\n",
    "RLPOP_min_delta = 0.0001 # порог изменения отслеживаемого значения\n",
    "RLPOP_cooldown = 3 # количество эпох до возобновления работы после изменения шага сходимости\n",
    "RLPOP_min_lr = 0 # минимальное значение шага сходимости\n",
    "\n",
    "    # для ModelCheckpoint\n",
    "MC_path = f\"networks/CNN/checkpoints/model_id=v{MODEL_VER}n{NEW_MODEL_NUM}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)\" + \"/check-{epoch:04d}.keras\"\n",
    "\n",
    "    # для CallbackList\n",
    "CBL_add_history = True # вызывать ли колбэк History (если он не был довавлен вручную)\n",
    "CBL_add_progbar = False # вызывать ли колбэк ProgbarLogger (если он не был довавлен вручную)\n",
    "\n",
    "    # для TensorBoard\n",
    "TB_log_dir = f\"networks/CNN/logs/model_id=v{MODEL_VER}n{NEW_MODEL_NUM}/\" # путь для сохранения логов процесса обучения модели\n",
    "TB_histogram_freq = 1 # частота сохранения логов'''\n",
    "\n",
    "#///////////////////////////////// для тренировки\n",
    "\n",
    "FIT_minibatch_size = 1 # для какого кол-ва наблюдений считать back propagation за раз\n",
    "FIT_buffer_size = FIT_minibatch_size * 2000 # сколько взять данных из генератора чтобы это влезло в оперативную память пк и видеокарты\n",
    "#FIT_shuffle = True # перемешивать ли данные\n",
    "FIT_verbose = True # выводить ли прогресс обучения в его процессее\n",
    "FIT_epochs = 4 # количество эпох обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b721b-6316-4a4d-b312-f0a7bb395e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = mv.get_model_v10(CROP_SIZE)\n",
    "'''DROP = 0.1\n",
    "\n",
    "def bnorm(layer, drop_percent):\n",
    "    #return Dropout(drop_percent)(BatchNormalization()(layer))\n",
    "    return Dropout(drop_percent)(layer)\n",
    "    #return BatchNormalization()(layer)'''\n",
    "\n",
    "augment_data = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  #layers.RandomRotation(1, fill_mode='reflect'),\n",
    "  layers.RandomTranslation(0.2,0.2, fill_mode=\"reflect\"),\n",
    "  #layers.RandomZoom(0.3,0.3, fill_mode=\"reflect\")\n",
    "])\n",
    "\n",
    "input_data = Input((16,16,64), name = 'input_data')\n",
    "\n",
    "aug_data = augment_data(input_data)\n",
    "\n",
    "# 12\n",
    "dconv_1_1 = Conv2D(128, (3,3), dilation_rate=(2, 2), activation='relu', name='dconv_1_1')(aug_data)\n",
    "\n",
    "# 6\n",
    "dconv_1_2 = Conv2D(128, (3,3), dilation_rate=(5, 5), activation='relu', name='dconv_1_2')(aug_data)\n",
    "up_1_2 = UpSampling2D(2, interpolation='bilinear', name='up_1_2') (dconv_1_2)\n",
    "\n",
    "# 4\n",
    "dconv_1_3 = Conv2D(128, (3,3), dilation_rate=(6, 6), activation='relu', name='dconv_1_3')(aug_data)\n",
    "up_1_3 = UpSampling2D(3, interpolation='bilinear', name='up_1_3') (dconv_1_3)\n",
    "\n",
    "# 2\n",
    "dconv_1_4 = Conv2D(128, (3,3), dilation_rate=(7, 7), activation='relu', name='dconv_1_4')(aug_data)\n",
    "up_1_4 = UpSampling2D(6, interpolation='bilinear', name='up_1_4') (dconv_1_4)\n",
    "\n",
    "# 12\n",
    "conc_1_1 = concatenate([dconv_1_1, up_1_2, up_1_3, up_1_4],axis=3, name='conc_1_1')\n",
    "\n",
    "conv_2_1 = Conv2D(512, (3,3), dilation_rate=(2, 2), activation='relu', padding='same', name='conv_2_1')(conc_1_1)\n",
    "conv_2_2 = Conv2D(512, (3,3), dilation_rate=(2, 2), activation='relu', padding='same', name='conv_2_2')(conv_2_1)\n",
    "conv_2_3 = Conv2D(512, (3,3), dilation_rate=(2, 2), activation='relu', padding='same', name='conv_2_3')(conv_2_2)\n",
    "pool_2_1 = MaxPooling2D((2,2), strides=2, name='pool_2_1')(conv_2_3)\n",
    "\n",
    "conv_2_4 = Conv2D(1024, (2,2), activation='relu', name='conv_2_4')(pool_2_1) # core (2,2)\n",
    "conv_2_5 = Conv2D(1024, (2,2), activation='relu', name='conv_2_5')(conv_2_4)\n",
    "conv_2_6 = Conv2D(1024, (2,2), activation='relu', name='conv_2_6')(conv_2_5)\n",
    "conv_2_7 = Conv2D(1024, (2,2), activation='relu', name='conv_2_7')(conv_2_6)\n",
    "pool_2_2 = MaxPooling2D((2,2), strides=2, name='pool_2_2')(conv_2_7)\n",
    "\n",
    "d_4_1 = Dense(1024, activation='relu', name='d_4_1')(Flatten(name='flat_3_1')(pool_2_2))\n",
    "d_4_2 = Dense(512, activation='relu', name='d_4_2')(d_4_1)\n",
    "d_4_3 = Dense(256, activation='relu', name='d_4_3')(d_4_2)\n",
    "d_4_4 = Dense(128, activation='relu', name='d_4_4')(d_4_3)\n",
    "d_4_5 = Dense(64, activation='relu', name='d_4_5')(d_4_4)\n",
    "d_4_6 = Dense(16, activation='relu', name='d_4_6')(d_4_5)\n",
    "d_4_7 = Dense(4, activation='relu', name='d_4_7')(d_4_6)\n",
    "\n",
    "output_def_bool = Dense(1, activation='sigmoid', name='output_def_bool')(d_4_7)\n",
    "    \n",
    "model = keras.Model([aug_data], [output_def_bool], name=f'id_v{MODEL_VER}n{NEW_MODEL_NUM}')\n",
    "\n",
    "#model = keras.models.load_model(f\"networks/CNN/id=v{MODEL_VER}n{OLD_MODEL_NUM}_in({CROP_SIZE}x{CROP_SIZE}x64)_out(1)_train=0dot0_test=0dot0.keras\")\n",
    "\n",
    "#model.compile(optimizer=CMP_solver, loss=CMP_loss_func, metrics=CMP_metrics)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbf854-df6a-4265-a2b0-7a45d6785544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=200,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126aab6-5765-4d3f-be9b-a57c9451e56b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''# Создание и настройка колбэков\n",
    "callback_list = [] # массив колбэков до подачи в колбек \"callbacklist\"\n",
    "\n",
    "callback_list.append(keras.callbacks.EarlyStopping(\n",
    "            monitor = ES_monitor_parametr, \n",
    "            min_delta = ES_min_delta, \n",
    "            patience = ES_patience,\n",
    "            restore_best_weights = ES_save_best_weights\n",
    "            ))\n",
    "\n",
    "callback_list.append(keras.callbacks.ModelCheckpoint(\n",
    "            MC_path, \n",
    "            verbose=1, \n",
    "            save_weights_only=False,\n",
    "            save_freq='epoch'))\n",
    "\n",
    "callback_list.append(keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = RLPOP_monitor_parametr, \n",
    "            factor = RLPOP_factor, \n",
    "            patience = RLPOP_patience, \n",
    "            verbose = RLPOP_verbose,\n",
    "            mode = RLPOP_mode, \n",
    "            min_delta = RLPOP_min_delta, \n",
    "            cooldown = RLPOP_cooldown, \n",
    "            min_lr = RLPOP_min_lr\n",
    "            ))\n",
    "\n",
    "callback_list.append(keras.callbacks.TensorBoard(\n",
    "            log_dir=TB_log_dir, \n",
    "            histogram_freq=TB_histogram_freq))\n",
    "\n",
    "FIT_callback_list = keras.callbacks.CallbackList(\n",
    "            callbacks = callback_list, \n",
    "            add_history = CBL_add_history, \n",
    "            add_progbar = CBL_add_progbar, \n",
    "            model = model\n",
    "            )''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df850a7e-9f25-4722-8d45-0145ebb7b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_array(arr: np.ndarray, bach_size: int):\n",
    "    for i in range(0, arr.shape[0], bach_size):\n",
    "        yield arr[i:i+bach_size]\n",
    "\n",
    "@tf.function\n",
    "def train_batch_step(train_batch_x, train_batch_y, train_batch_bin, model, loss, solver, metrics):\n",
    "    # bach propagation on minibatch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(train_batch_x, training=True)  # Logits for this minibatch\n",
    "        loss_value = loss(train_batch_bin, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    solver.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # metrics update\n",
    "    for metric in CMP_metrics:\n",
    "        metric.update_state(train_batch_bin, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_batch_step(val_batch_x, val_batch_y, val_batch_bin, model, loss, solver, metrics):\n",
    "    # bach propagation on minibatch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(val_batch_x, training=True)  # Logits for this minibatch\n",
    "        loss_value = loss(val_batch_bin, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    solver.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # metrics update\n",
    "    for metric in CMP_metrics:\n",
    "        metric.update_state(val_batch_bin, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aa881-684e-4919-a324-3072750e72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {metric.get_config()[\"name\"]:[] for metric in CMP_metrics} | {f'val_{metric.get_config()[\"name\"]}':[] for metric in CMP_metrics}\n",
    "for epoch in range(FIT_epochs):\n",
    "    start_time = time.time()\n",
    "    # get dataset generator for each epoch\n",
    "    dataset = {'train':dict(zip(['x','y','bin'], [dw.get_batch_generator(part,FIT_buffer_size) for part in chain_dataset_gens(dataset_desc['train'])] )),\n",
    "               'val':dict(zip(['x','y','bin'], [dw.get_batch_generator(part,FIT_buffer_size) for part in chain_dataset_gens(dataset_desc['val'])] ))}\n",
    "    # |||||||||||||||||||||||||||||||||||||||||||| train ||||||||||||||||||||||||||||||||||||||||||||\n",
    "    print(f'Epoch: {epoch+1}/{FIT_epochs}')\n",
    "    for train_buffer_x, train_buffer_y, train_buffer_bin in zip(dataset['train']['x'],dataset['train']['y'],dataset['train']['bin']):\n",
    "        for train_batch_x, train_batch_y, train_batch_bin in zip(get_batched_array(train_buffer_x, FIT_minibatch_size), \n",
    "                                                                 get_batched_array(train_buffer_y, FIT_minibatch_size), \n",
    "                                                                 get_batched_array(train_buffer_bin, FIT_minibatch_size)):\n",
    "            #bach propagation on minibatch\n",
    "            train_batch_step(train_batch_x, train_batch_y, train_batch_bin, model, CMP_loss_func, CMP_solver, CMP_metrics)\n",
    "            \n",
    "    print('TRAIN',end='\\t')        \n",
    "    for metric in CMP_metrics:\n",
    "        logs[metric.get_config()[\"name\"]].append(metric.result().numpy())\n",
    "        print(f'{metric.get_config()[\"name\"]}:{metric.result():.6f}', end=' ')\n",
    "        metric.reset_state()\n",
    "    print()\n",
    "    \n",
    "    # |||||||||||||||||||||||||||||||||||||||||||| val ||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for val_buffer_x, val_buffer_y, val_buffer_bin in zip(dataset['val']['x'],dataset['val']['y'],dataset['val']['bin']):\n",
    "        for val_batch_x, val_batch_y, val_batch_bin in zip(get_batched_array(val_buffer_x, FIT_minibatch_size), \n",
    "                                                           get_batched_array(val_buffer_y, FIT_minibatch_size), \n",
    "                                                           get_batched_array(val_buffer_bin, FIT_minibatch_size)):\n",
    "        \n",
    "            val_batch_step(val_batch_x, val_batch_y, val_batch_bin, model, CMP_loss_func, CMP_solver, CMP_metrics)\n",
    "            \n",
    "    print('VAL',end='\\t')     \n",
    "    for metric in CMP_metrics:\n",
    "        logs[f'val_{metric.get_config()[\"name\"]}'].append(metric.result().numpy())\n",
    "        print(f'{metric.get_config()[\"name\"]}:{metric.result():.6f}', end=' ')\n",
    "        metric.reset_state()\n",
    "\n",
    "    model.save(f\"{PATH_TO_SAVE_CHECKPOINTS}/check-{epoch:04}.keras\")\n",
    "    print()\n",
    "    print(f\"Time taken: {(time.time() - start_time):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd81234-8a49-4563-893d-247c6663e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранинеи процесса обучения в файл\n",
    "learn_df = pd.DataFrame.from_dict(logs)\n",
    "learn_df.to_excel(f'{PATH_TO_SAVE_LOGS}/learn_df.xlsx')\n",
    "display(learn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13acb56d-d5a6-4654-9239-97fbd02b0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_SIZE = 15\n",
    "for key1,key2 in zip(['BinaryAccuracy','Precision','MSE'],\n",
    "                     ['loss','Recall','PR_AUC']):\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "\n",
    "    fig.set_figwidth(22)\n",
    "    fig.set_figheight(8)\n",
    "    \n",
    "    axes[0].plot(logs[key1], \n",
    "             label='Train dataset',  linewidth=1.5, color='blue')\n",
    "    axes[0].plot(logs[f'val_{key1}'], linestyle = '--', \n",
    "             label='Validation dataset',  linewidth=3, color='red')\n",
    "    axes[0].set_xlabel('Epoch number', fontsize=FONT_SIZE)\n",
    "    axes[0].set_ylabel(f'{key1} value', fontsize=FONT_SIZE)\n",
    "    axes[0].set_title(f\"Learning process {key1} plot\", fontsize=FONT_SIZE, pad=15)\n",
    "    axes[0].tick_params(axis='both', which='both', labelsize = FONT_SIZE)\n",
    "    axes[0].minorticks_on()\n",
    "    axes[0].grid(which='major', linewidth=2)\n",
    "    axes[0].grid(which='minor', color = 'gray', linestyle = ':')\n",
    "\n",
    "    axes[1].plot(logs[key2], \n",
    "             label='Train dataset',  linewidth=1.5, color='blue')\n",
    "    axes[1].plot(logs[f'val_{key2}'], linestyle = '--', \n",
    "             label='Validation dataset',  linewidth=3, color='red')\n",
    "    axes[1].set_xlabel('Epoch number', fontsize=FONT_SIZE)\n",
    "    axes[1].set_ylabel(f'{key2} value', fontsize=FONT_SIZE)\n",
    "    axes[1].set_title(f\"Learning process {key2} plot\", fontsize=FONT_SIZE, pad=15)\n",
    "    axes[1].tick_params(axis='both', which='both', labelsize = FONT_SIZE)\n",
    "    axes[1].minorticks_on()\n",
    "    axes[1].grid(which='major', linewidth=2)\n",
    "    axes[1].grid(which='minor', color = 'gray', linestyle = ':')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76579d2-25dd-42f2-bbf6-0e5566aae134",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# quick check is tf.dataDataset is working in current tensorflow version\n",
    "'''dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "  print(element)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e85560-7943-44e4-9985-83a5a307c6f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RIGHT WAY to use generators in tf.data.Dataset instead of manual method on tensorflow level\n",
    "'''def train_dataset(epochs, batch, crop_size):\n",
    "    (train_x_time, \n",
    "     train_x_amp, \n",
    "     train_y_binary,\n",
    "     y_data_depth) = get_dataset_gen(PATH_TO_DATA['run_1'], 200, DataCrop(None,None,200,None), crop_size, 16)\n",
    "    \n",
    "    (train_x_time2, \n",
    "    train_x_amp2, \n",
    "    train_y_binary2,\n",
    "    y_data_depth2) = get_dataset_gen(PATH_TO_DATA['run_2'], 200, DataCrop(None,None,None,200), crop_size, 2)\n",
    "    \n",
    "    train_x_time = itertools.chain(train_x_time1,train_x_time2)\n",
    "    train_x_amp = itertools.chain(train_x_amp1,train_x_amp2)\n",
    "    train_y_binary = itertools.chain(train_y_binary1,train_y_binary2)\n",
    "    y_data_depth = itertools.chain(y_data_depth1,y_data_depth2)\n",
    "    \n",
    "    def my_generator(x_time_gen, x_amp_gen, y_binary_gen):\n",
    "        for time,amp,binary in zip(x_time_gen, x_amp_gen, y_binary_gen):\n",
    "            yield (time, amp), binary\n",
    "            \n",
    "    dataset = tf.data.Dataset.from_generator(lambda: my_generator(train_x_time, train_x_amp, train_y_binary),\n",
    "                                             output_signature=(\n",
    "                                                 (tf.TensorSpec(shape=(crop_size,crop_size,32), dtype=tf.float64),\n",
    "                                                  tf.TensorSpec(shape=(crop_size,crop_size,32), dtype=tf.float64)),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.bool)))\n",
    "\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch)\n",
    "    \n",
    "    return dataset'''\n",
    "\n",
    "'''def val_dataset(epochs, batch, crop_size):\n",
    "    (val_x_time, \n",
    "     val_x_amp, \n",
    "     val_y_binary,\n",
    "     y_data_depth) = get_dataset_gen(PATH_TO_DATA['run_1'], 200, DataCrop(None,None,None,200), crop_size, 2)\n",
    "    \n",
    "    def my_generator(x_time,x_amp,y_binary):\n",
    "        for time,amp,binary in zip(val_x_time, val_x_amp, val_y_binary):\n",
    "            yield (time, amp), binary\n",
    "            \n",
    "    dataset = tf.data.Dataset.from_generator(lambda: my_generator(val_x_time, val_x_amp, val_y_binary),\n",
    "                                             output_signature=(\n",
    "                                                 (tf.TensorSpec(shape=(crop_size,crop_size,32), dtype=tf.float64),\n",
    "                                                  tf.TensorSpec(shape=(crop_size,crop_size,32), dtype=tf.float64)),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.bool)))\n",
    "\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch)\n",
    "    \n",
    "    return dataset''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b3189-a635-46c3-9420-24717863c2b2",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

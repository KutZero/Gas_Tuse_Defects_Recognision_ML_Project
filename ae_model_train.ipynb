{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde58157-7750-45b9-a343-b46e3c74b5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dependencies import\n",
    "from common_dependencies import *\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "logger = logging.getLogger(f'main.ae_model_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a059ca0-3552-4d9d-b6c9-ad81183cf0b1",
   "metadata": {},
   "source": [
    "# Чтение и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067be933-41ad-444d-9364-c01206571966",
   "metadata": {},
   "source": [
    "### Параметры обучающих выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf461c0-86ef-4815-a965-1e766c1f33b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train params that can be easily changed\n",
    "MAX_VAL = 1000 # не трогать\n",
    "XSHIFT = 200 # не трогать\n",
    "\n",
    "# настройка параметров выборок\n",
    "dataset_desc = {'train': (DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(0,0,MAX_VAL,60),SlidingCrop(1,1),XSHIFT),\n",
    "                          DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(0,0,MAX_VAL,60),SlidingCrop(1,1),XSHIFT),\n",
    "                         ),\n",
    "                'val': (DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(0,60,MAX_VAL,20),SlidingCrop(1,1),XSHIFT),\n",
    "                        DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(0,60,MAX_VAL,20),SlidingCrop(1,1),XSHIFT)\n",
    "                       ),\n",
    "                'test': (DatasetPartDescription(PATH_TO_DATA['run_1'],DataCrop(0,80,MAX_VAL,MAX_VAL),SlidingCrop(1,1),XSHIFT),\n",
    "                         DatasetPartDescription(PATH_TO_DATA['run_2'],DataCrop(0,80,MAX_VAL,MAX_VAL),SlidingCrop(1,1),XSHIFT)\n",
    "                        )}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50df50a-5047-4f11-91a2-f371bbc16431",
   "metadata": {},
   "source": [
    "### Чтение и подготовка данных обучающих выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad20a11-4c77-4a15-84db-632679b177b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#входные и выходные данные\n",
    "# reading\n",
    "dataset = {'train':dict(zip(['x','y','bin'], [np.array(list(gen)) for gen in chain_dataset_gens(dataset_desc['train'])])),\n",
    "           'val':dict(zip(['x','y','bin'], [np.array(list(gen)) for gen in chain_dataset_gens(dataset_desc['val'])])), \n",
    "           'test':dict(zip(['x','y','bin'], [np.array(list(gen)) for gen in chain_dataset_gens(dataset_desc['test'])]))}\n",
    "\n",
    "# squueze datasets\n",
    "for dataset_part_name, dataset_part in dataset.items():\n",
    "    for data_part_name, data_part in dataset_part.items():\n",
    "        if data_part_name == 'x':\n",
    "            dataset[dataset_part_name][data_part_name] = data_part.reshape(-1,64)\n",
    "        if data_part_name == 'y':\n",
    "            dataset[dataset_part_name][data_part_name] = data_part.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8559eb-f39c-4f44-ab11-48ad14432a9e",
   "metadata": {},
   "source": [
    "### Размерности данных в обучающих выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c94b8-eb76-4bd3-a7aa-37e93ae5718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying\n",
    "logger.debug('Dataset')\n",
    "for dataset_part_name, dataset_part in dataset.items():\n",
    "    logger.debug('|'*8+dataset_part_name+'|'*8)\n",
    "    for data_part_name, data_part in dataset_part.items():\n",
    "        logger.debug(f'{data_part_name}.shape: {data_part.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3b24c-3e09-4b10-8a3b-80b23f84cb24",
   "metadata": {},
   "source": [
    "### Какие части 2 файлов с данными относятся к конкретным выборкам (к тренировочной, тестовой, валидационной)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5a53d-3393-4d46-8066-7a4a5f9d495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show parts took for learning\n",
    "all_rects = {'run_1': {'train':None,'val':None,'test':None}, \n",
    "             'run_2': {'train':None,'val':None,'test':None}}\n",
    "rects_colors = {'train':'red', 'val':'green', 'test':'yellow'}\n",
    "\n",
    "for run_name in all_rects.keys():\n",
    "    x_df, y_df = dw.get_x_and_y_data(*PATH_TO_DATA[run_name])\n",
    "    x_df = None\n",
    "    y_df = dw.roll_df(y_df, XSHIFT, 1)\n",
    "    for dataset_part_name in all_rects[run_name].keys():\n",
    "        # get all DatasetPartDescription for train, val or test\n",
    "        dataset_part_desc = dataset_desc[dataset_part_name]\n",
    "        # get all DatasetPartDescription for current run_name (run_1 or run_2)\n",
    "        dataset_part_desc = [dataset_part for dataset_part in dataset_part_desc if re.findall(r'run_\\d', dataset_part.data_path_tuple[0])[0] == run_name]\n",
    "        # put rects list to all_rects[run_name][dataset_part_name]\n",
    "        all_rects[run_name][dataset_part_name] = [Rectangle((dataset_part.file_data_crop.left, dataset_part.file_data_crop.top), \n",
    "                           dataset_part.file_data_crop.width, dataset_part.file_data_crop.height, \n",
    "                           facecolor=rects_colors[dataset_part_name], alpha=0.5) for dataset_part in dataset_part_desc]\n",
    "    res_rects = list(itertools.chain(*[run_rects for run_rects_name, run_rects in all_rects[run_name].items()]))\n",
    "    if res_rects:\n",
    "        dw.draw_defects_map_with_rectangles_owerlap(y_df, res_rects, title = f'The parts took for learning from {run_name} (red - train, green - validate, yellow - test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f26eb-52a9-4835-bacb-f907f8197bb3",
   "metadata": {},
   "source": [
    "# Создание и обучение модели автокодировщика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3230bf-e3f5-4742-8d6d-9d9c4800c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# архитектура модели\n",
    "def get_model(learning_rate):\n",
    "    #CMP_learning_rate = 0.005 #0.0000002 # шаг сходимости back propogation\n",
    "    CMP_solver = keras.optimizers.Adam(learning_rate) # оптимизатор\n",
    "    CMP_loss_funcs = keras.losses.MeanSquaredError() #BinaryCrossentropy() \n",
    "    CMP_metrics = [keras.metrics.MeanSquaredError(name='MeanSquaredError')]\n",
    "    \n",
    "    HIDDEN_ACTIVATION = 'sigmoid' #'relu'\n",
    "    OUTPUT_ACTIVATION = 'sigmoid'\n",
    "    \n",
    "    enc_input = layers.Input((64,), name='enc_input')\n",
    "    d_1_1 = layers.Dense(64, activation=HIDDEN_ACTIVATION)(enc_input)\n",
    "    \n",
    "    d_2_1 = layers.Dense(32, activation=HIDDEN_ACTIVATION)(d_1_1)\n",
    "    d_2_2 = layers.Dense(32, activation=HIDDEN_ACTIVATION)(d_1_1)\n",
    "    #d_2_3 = layers.Dense(8, activation=HIDDEN_ACTIVATION)(d_1_1)\n",
    "\n",
    "    #d_4_1 = layers.Dense(16, activation=HIDDEN_ACTIVATION)(concatenate([d_2_1, d_2_2], axis=1))\n",
    "    #d_4_3 = layers.Dense(32, activation=HIDDEN_ACTIVATION)(d_1_1)\n",
    "\n",
    "    hidden_state_output = layers.Dense(8, activation=HIDDEN_ACTIVATION, name='hidden_state_output')(concatenate([d_2_1, d_2_2], axis=1))\n",
    "\n",
    "    #d_5_1 = layers.Dense(16, activation=HIDDEN_ACTIVATION)(hidden_state_output)\n",
    "    \n",
    "\n",
    "    d_7_1 = layers.Dense(32, activation=HIDDEN_ACTIVATION)(hidden_state_output)\n",
    "    d_7_2 = layers.Dense(32, activation=HIDDEN_ACTIVATION)(hidden_state_output)\n",
    "    #d_7_3 = layers.Dense(8, activation=HIDDEN_ACTIVATION)(d_6_2)\n",
    "\n",
    "\n",
    "    dec_output = layers.Dense(64, activation=OUTPUT_ACTIVATION, name='dec_output')(concatenate([d_7_1,d_7_2], axis=1))\n",
    "    \n",
    "    model = keras.Model(enc_input, dec_output, name='ae')\n",
    "    model.compile(optimizer=CMP_solver, loss=CMP_loss_funcs, metrics=CMP_metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d20658-5e0a-4bea-9b20-ad18ac43fe89",
   "metadata": {},
   "source": [
    "### Вывести параметры модели и её граф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da92fca-9a8e-443d-b021-bf953280d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(0.005)\n",
    "print(model.summary())\n",
    "'''tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=200,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False,\n",
    ")''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d91a2-ab04-4ade-b0b7-f1cf97742615",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392190-9117-4bd1-a356-4b85b9d7b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# до какого размера кодируются данные\n",
    "# или по имени слоя. Обычно \"hidden_state_output\"\n",
    "# model.get_layer('hidden_state_output').output.shape[1]\n",
    "ENCODED_SIZE = min([layer.output.shape[1] for layer in model.layers]) \n",
    "# размер входных и выходных данных\n",
    "DECODED_SIZE = model.layers[-1].output.shape[1]\n",
    "PATH_TO_SAVE_MODEL = pathlib.Path(f'networks/AE/encoded_to_{ENCODED_SIZE}')\n",
    "PATH_TO_SAVE_MODEL_PROGRESS = PATH_TO_SAVE_MODEL/'logs'\n",
    "MODEL_VERSION = 1\n",
    "MODEL_NUMBER = 1\n",
    "MIN_TRAIN_LOSS = 1\n",
    "MIN_VAL_LOSS = 1\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_MODEL):\n",
    "    os.makedirs(PATH_TO_SAVE_MODEL)\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_MODEL_PROGRESS):\n",
    "    os.makedirs(PATH_TO_SAVE_MODEL_PROGRESS)\n",
    "\n",
    "# все имеющиеся модели с такими же ENCODED_SIZE и DECODED_SIZE\n",
    "all_ae_models = [path.name for path in PATH_TO_SAVE_MODEL.parent.rglob('*.keras') \n",
    "                 if re.search(fr'in\\({DECODED_SIZE}\\)_hid\\({ENCODED_SIZE}\\)', path.name)]\n",
    "\n",
    "# если уже есть такая же архитектура модели сделать MODEL_VERSION такой же\n",
    "# а MODEL_NUMBER на 1 больше чем имеющаяся\n",
    "if all_ae_models:\n",
    "    min_train_loss = min([float(re.findall(fr'train=(.+),val', name)[0]) for name in all_ae_models])\n",
    "    min_val_loss = min([float(re.findall(fr',val=(.+),test', name)[0]) for name in all_ae_models])\n",
    "    min_model_number = max([int(re.findall(fr'id=v{MODEL_VERSION:04}n(\\d+)_', name)[0]) for name in all_ae_models])\n",
    "\n",
    "    if MODEL_NUMBER <= min_model_number:\n",
    "        MODEL_NUMBER = min_model_number+1\n",
    "        \n",
    "    if MIN_TRAIN_LOSS >= min_train_loss:\n",
    "        MIN_TRAIN_LOSS = min_train_loss\n",
    "\n",
    "    if MIN_VAL_LOSS >= min_val_loss:\n",
    "        MIN_VAL_LOSS = min_val_loss\n",
    "\n",
    "print(f'{ENCODED_SIZE=}')\n",
    "print(f'{DECODED_SIZE=}')\n",
    "print(f'{PATH_TO_SAVE_MODEL=}')\n",
    "print(f'{MODEL_VERSION=}')\n",
    "print(f'{MODEL_NUMBER=}')\n",
    "print(f'{MIN_TRAIN_LOSS=}')\n",
    "print(f'{MIN_VAL_LOSS=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6d864-b783-4dad-b9f6-3726243f4449",
   "metadata": {},
   "source": [
    "### Настройка и создание коллбэков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74e1b4-c3bb-465d-b80f-a3246e00ff42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback_params = {\n",
    "    # остановка обучения если модель перестала учиться\n",
    "    'EarlyStopping': {\n",
    "        'monitor': 'val_loss', # отслеживаемый параметр \n",
    "        'min_delta': 0.00001, # минимальное улучшение параметра за cur_patience\n",
    "        'patience': 6, # кол-во эпох без улучшений\n",
    "        'restore_best_weights': False,  # сохранять ли веса нейронки с лучшими результатами\n",
    "    },\n",
    "\n",
    "    # уменьшение шага сходимости, если модель стала мендленно учиться\n",
    "    'ReduceLROnPlateau': {\n",
    "        'monitor' : 'loss', # отслеживаемый параметр \n",
    "        'factor' : 0.2, # множитель для расчета нового шага сходимости (new_learning_rate = old_learning_rate*RLPOP_factor)\n",
    "        'patience' : 3, # кол-во эпох без улучшений\n",
    "        'verbose' : 0, # выводить ли прогресс изменения шага сходимости в его процессее\n",
    "        'min_delta' : 0.0001, # порог изменения отслеживаемого значения\n",
    "        'cooldown' : 1, # количество эпох до возобновления работы после изменения шага сходимости\n",
    "        'min_lr' : 0# минимальное значение шага сходимости\n",
    "    },\n",
    "}\n",
    "\n",
    "# запись процесса обучения в логи\n",
    "''''TensorBoard': {\n",
    "    'log_dir' : PATH_TO_SAVE_MODEL/'logs',\n",
    "    'histogram_freq' : 1,\n",
    "    'write_images' : 1,\n",
    "    'embeddings_freq' : 1\n",
    "}''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ec2ca-10a3-4eda-aca5-2a52a7db7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и настройка колбэков\n",
    "\n",
    "callback_list = [] # массив колбэков до подачи в колбек \"callbacklist\"\n",
    "\n",
    "# остановка обучения если модель перестала учиться\n",
    "callback_list.append(keras.callbacks.EarlyStopping(**callback_params['EarlyStopping']))\n",
    "\n",
    "# уменьшение шага сходимости, если модель стала мендленно учиться\n",
    "callback_list.append(keras.callbacks.ReduceLROnPlateau(**callback_params['ReduceLROnPlateau']))\n",
    "\n",
    "# запись процесса обучения в логи\n",
    "#callback_list.append(keras.callbacks.TensorBoard(**callback_params['TensorBoard']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a50d03-1745-4965-b962-b4f016d3cf44",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6f620-b611-4c93-a6b6-510df358561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(0, 200, 10):\n",
    "    print(f'Seed: {seed}',\"|\"*10)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    for lrate in [0.01,0.005,0.0025]:\n",
    "        print(f'Start learning rate: {lrate}',\"|\"*5)\n",
    "        for batch_size in [8,16,32,64]:\n",
    "            print(f'\\tBatch size: {batch_size}')\n",
    "\n",
    "            # get model\n",
    "            model = get_model(lrate)\n",
    "            \n",
    "            history = model.fit(dataset['train']['x'], dataset['train']['x'],\n",
    "                            batch_size = batch_size, \n",
    "                            epochs = 60, \n",
    "                            verbose = 0, \n",
    "                            shuffle = True,\n",
    "                            validation_data = (dataset['val']['x'], dataset['val']['x']), \n",
    "                            callbacks = callback_list)\n",
    "            cur_test_loss = model.evaluate(dataset['test']['x'], dataset['test']['x'], \n",
    "                                      batch_size=batch_size, verbose=0)[0]\n",
    "\n",
    "            cur_train_loss = history.history['loss'][-1]\n",
    "            cur_val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "            model_name = (\n",
    "                f\"id=v{MODEL_VERSION:04}n{MODEL_NUMBER:04}\" +\n",
    "                f\"_in({DECODED_SIZE})_hid({ENCODED_SIZE})\" + \n",
    "                f\"_loss_MSE=(train={cur_train_loss:.5f},\" + \n",
    "                f\"val={cur_val_loss:.5f},test={cur_test_loss:.5f})\" + \n",
    "                f\"_seed={seed}_lrate={lrate}_bach_size={batch_size}\")\n",
    "\n",
    "\n",
    "            if cur_train_loss < MIN_TRAIN_LOSS and cur_val_loss < MIN_VAL_LOSS:\n",
    "                MIN_TRAIN_LOSS = cur_train_loss\n",
    "                MIN_VAL_LOSS = cur_val_loss\n",
    "                cont = True\n",
    "            if cur_val_loss < MIN_VAL_LOSS:\n",
    "                MIN_VAL_LOSS = cur_val_loss\n",
    "                cont = True\n",
    "            if cur_train_loss < MIN_TRAIN_LOSS:\n",
    "                MIN_TRAIN_LOSS = cur_train_loss\n",
    "                cont = True\n",
    "                \n",
    "            print(f'\\t\\tEpochs: {len(history.history[\"loss\"])}')\n",
    "            print(f\"\\t\\tloss_MSE=(train={cur_train_loss:.5f},val={cur_val_loss:.5f},test={cur_test_loss:.5f})\")\n",
    "            \n",
    "            if cont:\n",
    "                learn_df = pd.DataFrame.from_dict(history.history)\n",
    "                callback_df = pd.DataFrame.from_dict(callback_params)\n",
    "                dataset_desc_df = pd.DataFrame.from_dict(dataset_desc)\n",
    "                dataset_df = pd.DataFrame.from_dict({part_name: {item_name: item.shape for (item_name, item) in part.items()} \n",
    "                                                         for (part_name, part) in dataset.items()})\n",
    "                with pd.ExcelWriter(PATH_TO_SAVE_MODEL_PROGRESS/f\"{model_name}_learning_data.xlsx\") as writer:\n",
    "                    learn_df.to_excel(writer, sheet_name = 'learning_progress')\n",
    "                    callback_df.to_excel(writer, sheet_name = 'callback_params')\n",
    "                    dataset_desc_df.to_excel(writer, sheet_name = 'dataset_reading_params')\n",
    "                    dataset_df.to_excel(writer, sheet_name = 'prepared_dataset_params')\n",
    "                    \n",
    "                model.save(PATH_TO_SAVE_MODEL/f'{model_name}.keras')\n",
    "                \n",
    "                MODEL_NUMBER+=1\n",
    "                cont=False\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550d142-34e6-425f-9db0-144c2c38ab30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''#id=v13n01_in(64)_hid(16)_loss_MSE=(train=0.00077,val=0.00054,test=0.00075)\n",
    "MODEL_VER = 15\n",
    "min_train_loss = 0.00141\n",
    "min_val_loss = 0.00098\n",
    "num=1\n",
    "for i in range(0,1000,10):\n",
    "    tf.compat.v1.set_random_seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    np.random.seed(i)\n",
    "    print(f'Seed: {i}',\"|\"*10)\n",
    "    for learning_rate in [0.01,0.005]:\n",
    "        print(f'Start learning rate: {learning_rate}',\"|\"*5)\n",
    "        for batch_size in [32,]:\n",
    "            print(f'\\tBatch size: {batch_size}')\n",
    "\n",
    "\n",
    "            model = get_model()\n",
    "            history = model.fit(dataset['train']['x'], dataset['train']['x'],\n",
    "                            batch_size = batch_size, \n",
    "                            epochs = 60, \n",
    "                            verbose = 0, \n",
    "                            shuffle=True,\n",
    "                            validation_data = (dataset['val']['x'], dataset['val']['x']), \n",
    "                            callbacks = callback_list)\n",
    "            res_loss = model.evaluate(dataset['test']['x'], dataset['test']['x'], batch_size = 32, verbose=0)\n",
    "            \n",
    "            cont = False\n",
    "\n",
    "            model_name = f\"\"\"networks/AE/id=v{MODEL_VER:02}n{num:02}_in(64)_hid({min([layer.output.shape[1] for layer in model.layers])})_loss_MSE=\n",
    "            (train={history.history['loss'][-1]:.5f},\n",
    "            val={history.history['val_loss'][-1]:.5f},\n",
    "            test={res_loss:.5f}).keras\"\"\"\n",
    "            \n",
    "            if history.history['loss'][-1] < min_train_loss and history.history['val_loss'][-1] < min_val_loss:\n",
    "                min_train_loss = history.history['loss'][-1]\n",
    "                min_val_loss = history.history['val_loss'][-1]\n",
    "                cont = True\n",
    "            if history.history['val_loss'][-1] < min_val_loss:\n",
    "                min_val_loss = history.history['val_loss'][-1]\n",
    "                cont = True\n",
    "            if history.history['loss'][-1] < min_train_loss:\n",
    "                min_train_loss = history.history['loss'][-1]\n",
    "                cont = True\n",
    "                \n",
    "            print(f'\\tEpochs: {len(history.history[\"loss\"])}')\n",
    "            print(f\"\\tloss_MSE=(train={history.history['loss'][-1]:.5f},val={history.history['val_loss'][-1]:.5f},test={res_loss:.5f})\")\n",
    "            \n",
    "            \n",
    "            if cont:\n",
    "                model.save(model_name)\n",
    "                num+=1\n",
    "                cont=False\n",
    "                #print(f'Start learning rate: {learning_rate}',\"|\"*5)\n",
    "                #print(f'Batch size: {batch_size}')\n",
    "                #print(f'Epochs: {len(history.history[\"loss\"])}')\n",
    "                #print(f\"loss_MSE=(train={history.history['loss'][-1]:.5f},val={history.history['val_loss'][-1]:.5f},test={res_loss:.5f})\")\n",
    "                #print()\n",
    "                continue\n",
    "    print()'''\n",
    "\n",
    "'''\n",
    "Seed: 210 ||||||||||\n",
    "Start learning rate: 0.005 |||||\n",
    "Batch size: 16\n",
    "Epochs: 24\n",
    "loss_MSE=(train=0.00141,val=0.00098,test=0.00138)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edd1a3-0723-4be3-8931-86cdc89fefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''name = 'id=v05n01_in(64)_hid(32)_loss_MSE=(train=0.00027,val=0.00023,test=0.00029)'\n",
    "print(re.search(fr'hid\\({ENCODED_SIZE}\\)', name))\n",
    "print([int(item) for item in re.findall(fr'id=v(\\d+)n(\\d+)', name)[0]])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fdbc6-62c7-4909-bad9-bf2b9bfd7ebd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''HP_SEED = hp.HParam('seed', hp.Discrete(np.arange(0,1000,10)))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete(0.0025, 0.005))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete(np.arange(2,10,2)**2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(PATH_TO_SAVE_MODEL/'logs'/'hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_SEED, HP_LEARNING_RATE, HP_BATCH_SIZE, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric('mean_squarred_error', display_name='MSE')],\n",
    "  )''';\n",
    "\n",
    "'''def train_test_model(hparams, dataset, callback_list):\n",
    "  model = get_model(hparams[HP_LEARNING_RATE])\n",
    "    \n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "    model.fit(dataset['train']['x'], dataset['train']['x'],\n",
    "              batch_size = batch_size, \n",
    "              epochs = 60, \n",
    "              verbose = 0, \n",
    "              shuffle=True,\n",
    "              validation_data = (dataset['val']['x'], dataset['val']['x']), \n",
    "              callbacks = callback_list)\n",
    "    \n",
    "  return model.evaluate(dataset['test']['x'], dataset['test']['x'])''';\n",
    "\n",
    "'''def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615a1e2-0639-48a9-bb44-9356b4bd58c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''FONT_SIZE = 15\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(8)\n",
    "\n",
    "ax.plot(history.history['loss'], \n",
    "         label='Train dataset',  linewidth=1.5, color='blue')\n",
    "ax.plot(history.history[f'val_loss'], linestyle = '--', \n",
    "     label='Validation dataset',  linewidth=3, color='red')\n",
    "ax.set_xlabel('Epoch number', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(f'Loss value', fontsize=FONT_SIZE)\n",
    "ax.set_title(f\"Learning process loss plot. Test dataset value = {res_loss:.4f} (MSE)\", fontsize=FONT_SIZE, pad=15)\n",
    "\n",
    "ax.patch.set_alpha(0)\n",
    "\n",
    "#  Устанавливаем форматирование делений:\n",
    "ax.tick_params(axis='both', which='both', labelsize = FONT_SIZE)\n",
    "\n",
    "# Вывод и настройка сетки\n",
    "ax.minorticks_on()\n",
    "ax.grid(which='major', linewidth=2)\n",
    "ax.grid(which='minor', color = 'gray', linestyle = ':')\n",
    "\n",
    "ax.legend(fontsize = FONT_SIZE, facecolor = \"white\", loc = 'upper right')\n",
    "\n",
    "plt.show()''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
